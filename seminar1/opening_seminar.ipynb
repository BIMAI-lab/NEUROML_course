{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation & Overview\n",
    "\n",
    "- Modern neuroimaging technologies (EEG, fNIRS, MRI) generate massive and complex data.  \n",
    "- Machine Learning (ML) methods surged in neuroimaging research, going beyond classical univariate statistics.  \n",
    "- ML models can now distinguish neurological/psychiatric patients from controls, predict Alzheimer’s, schizophrenia, autism.  \n",
    "- Importance: pattern recognition, biomarker discovery, clinical decision support.  \n",
    "\n",
    "\n",
    "## Neuroimaging Modalities\n",
    "\n",
    "- **EEG (Electroencephalography).**  \n",
    "  Provides excellent temporal resolution (milliseconds) but limited spatial precision. It is highly sensitive to artifacts such as eye blinks, muscle noise, and electromagnetic interference. Common preprocessing steps include frequency filtering (low-pass/high-pass) and artifact removal methods (e.g., ICA). ML approaches on EEG have successfully classified cognitive traits.\n",
    "\n",
    "  **Example Study:**  \n",
    "  Mikheev et al. (2024) investigated EEG patterns during arithmetic, logical, and verbal tasks, applying ML models (logistic regression, Riemann projections, LightGBM with handcrafted spectral power features and explainability via SHAP) to distinguish individuals with mathematics vs. humanities background, achieving balanced accuracies between 0.84–0.89.  \n",
    "  [Read the article (Scientific Reports)](https://doi.org/10.1038/s41598-024-55163-w)\n",
    "\n",
    "  **Python libraries:**  \n",
    "  - [MNE-Python](https://mne.tools/) – preprocessing, ICA, filtering  \n",
    "  - [pyriemann](https://pyriemann.readthedocs.io/) – methods on SPD matrices  \n",
    "  - [scikit-learn](https://scikit-learn.org/stable/) – various classifiers  \n",
    "  - [TensorFlow/Keras](https://www.tensorflow.org/) – deep learning  \n",
    "  - [PyTorch](https://pytorch.org/) – deep learning\n",
    "\n",
    "\n",
    "\n",
    "- **MRI / fMRI.**  \n",
    "  Structural MRI provides high spatial resolution anatomical imaging; fMRI captures BOLD signals with ~second-level delays. fMRI data are large (millions of voxels) and susceptible to noise from subject motion, physiological fluctuations (e.g. breathing, heartbeat), and scanner drift. The choice of preprocessing pipeline can substantially alter results.\n",
    "\n",
    "  **Example Study:**  \n",
    "  Luppi et al. (2024) systematically evaluated 768 fMRI data-processing pipelines for resting-state functional connectomics and found that most pipelines produced inconsistent or misleading network reconstructions. A subset demonstrated robust performance across datasets and evaluation criteria.  \n",
    "  [Read the article (Nature Communications)](https://doi.org/10.1038/s41467-024-48781-5)\n",
    "\n",
    "  **Python libraries:**  \n",
    "  - [Nilearn](https://nilearn.github.io/) – machine learning for neuroimaging data  \n",
    "  - [NiBabel](https://nipy.org/nibabel/) – neuroimaging file handling  \n",
    "  - [fMRIPrep](https://fmriprep.org/) – standardized preprocessing pipeline  \n",
    "  - [Nipype](https://nipype.readthedocs.io/) – workflow orchestration\n",
    "\n",
    "\n",
    "\n",
    "- **fNIRS (Functional Near-Infrared Spectroscopy).**  \n",
    "  Measures cortical hemodynamics via optical signals. Advantages include portability and robustness to head motion (suitable for children), while limitations involve shallow penetration depth and sensitivity to physiological noise (pulse, respiration).\n",
    "\n",
    "  **Python libraries:**  \n",
    "  - [MNE-NIRS](https://mne.tools/mne-nirs/stable/index.html) – fNIRS integration with MNE  \n",
    "\n",
    "\n",
    "Each modality generates distinct data types—multi-channel time series (EEG), 3D voxel volumes (fMRI), or optical signals (fNIRS)—and requires tailored preprocessing strategies to address modality-specific noise sources and artifacts.\n",
    "\n",
    "\n",
    "## Machine Learning in Neuroimaging\n",
    "\n",
    "- **Supervised learning:** Classification (patient vs. control), regression (predicting scores).  \n",
    "- **Unsupervised learning:** Discover hidden clusters, connectivity subtypes.  \n",
    "- **Contrast with classical stats:** GLM tests voxels independently; ML captures multivariate patterns.  \n",
    "\n",
    "**Common ML Approaches:**  \n",
    "- Feature engineering + SVM/Random Forest.  \n",
    "- Deep Learning (CNNs, autoencoders) → segmentation, disease prediction.  \n",
    "- Validation: Proper CV, avoiding overfitting, ensuring generalization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Reminder of the Classical Pipeline - ML task\n",
    "\n",
    "Let $X$ is `data samples`, $Y$ - `targets`.  $y : X \\rightarrow Y$ is an unknown target function.\n",
    "\n",
    "`Input:`\n",
    "* $\\{x_1, \\dots, x_l\\} \\subset X$ - training sample;\n",
    "* $y_i = y(x_i), ~i = 1, \\dots, l$ - targets.\n",
    "\n",
    "\n",
    "`Output:`\n",
    "\n",
    "* $a: X \\rightarrow Y$ - `predicted function` close to $y$ on all set $X$.\n",
    "\n",
    "## How objects are set. Feature description\n",
    "\n",
    "$f_j$ - features of objects.\n",
    "\n",
    "`Types of features:`\n",
    "* Binary feature $f_j$:\n",
    "    * gender, headache, weakness, nausea, etc.\n",
    "* $f_j$ - categorical feature:\n",
    "    * name of the medicine\n",
    "* $f_j$ is an ordinal feature:\n",
    "    * severity of the condition, jaundice, etc.\n",
    "* Quantitative feature:\n",
    "    * age, pulse, blood pressure, hemoglobin content in the blood, dose of the drug, etc.\n",
    "\n",
    "\n",
    "`Vector` $\\big(f_1(x), f_2(x), \\ldots, f_n(x)\\big)$ is a feature description of the object $x \\in X$.\n",
    "\n",
    "The feature data is set as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "F =\n",
    "\\begin{pmatrix}\n",
    "    f_1(x_1) & \\dots & f_n(x_1) \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    f_1(x_ℓ) & \\dots & f_n(x_\\ell)\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classification model:\n",
    "\n",
    "`Train sample:` $X^\\ell = \\big(x_i,~y_i\\big)_{i=1}^{\\ell}, \\quad x_i \\in \\mathbb{R}^n, \\quad y_i \\in \\{-1,~+1\\}$\n",
    "\n",
    "\n",
    "* Classification model - __linear__:\n",
    "\\begin{equation}\n",
    "  a(x, θ) = \\text{sign} \\big(\\sum_{j=1}^{n} θ_j f_j(x)\\big), \\quad θ \\in \\mathbb{R}^n\n",
    "\\end{equation}\n",
    "\n",
    "* The loss function is __binary__ or its __approximation__::\n",
    "\\begin{equation}\n",
    "  \\mathscr{L}(a,~y) = [ay < 0] = \\big[x^\\top θ \\cdot y < 0\\big] \\le \\mathscr{L}\\big(x^\\top θ \\cdot y)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of classification errors\n",
    "\n",
    "\n",
    "The task of classification into `two classes`, $y_i\\in \\{-1,~+1\\}$.\n",
    "\n",
    "`Classification algorithm` $a(x_i) \\in \\{-1,~+1\\}$.\n",
    "\n",
    "By applying the algorithm $a(x)$ to objects $x$, we can get $4$ possible situations:\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1zfrRBSZhAs3RZeLYhUKeo0RsDPpPXjm-)\n",
    "\n",
    "`Positive / Negative` - which answer was given by the classifier $a(x)$. `True / False` - the classifier gave the correct answer or made a mistake.\n",
    "\n",
    "\n",
    "__Number of correct classifications__ (the more, the better):\n",
    "\\begin{equation}\n",
    "  \\text{Accuracy} = \\frac{1}{\\ell}\\sum_{i=1}^{\\ell}\\big[a(x_i) = y_i\\big] = ~\\frac{\\text{TP} + \\text{TN}}{\\text{FP} + \\text{FN} + \\text{TP} + \\text{TN}}\n",
    "\\end{equation}\n",
    "\n",
    "`!!! Disadvantage !!!`: does not take into account either the number (imbalance) of classes, or the cost of an error on objects of different classes.\n",
    "\n",
    "__For example__: Classification of patients vs. healthy controls\n",
    "\t•\tSuppose you have 100 subjects: 95 healthy and 5 patients with a rare neurological disorder.\n",
    "\t•\tA classifier that always predicts “healthy” will yield an accuracy of 95%.\n",
    "\t•\tAt first glance, 95% seems like high performance, but the model never identified a single patient — which was the key task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    ":class: dropdown\n",
    "**Supervised vs Unsupervised Learning in Neuroimaging **  \n",
    "1. Provide **one concrete example** of supervised learning in neuroimaging (e.g., patient vs. control classification or regression of cognitive scores).   \n",
    "2. Provide **one concrete example** of unsupervised learning in neuroimaging (e.g., discovering hidden clusters or connectivity subtypes). \n",
    "3. Explain how machine learning approaches differ from classical GLM-based studies that analyze each voxel independently. Highlight at least **two advantages** of capturing multivariate patterns.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 2\n",
    ":class: dropdown \n",
    "**Feature Engineering vs Deep Learning: Choosing the Right Tool**  \n",
    "1. Describe a scenario in neuroimaging where a traditional pipeline—feature engineering combined with SVM or Random Forest—is the preferred approach. \n",
    "2. Describe a scenario where deep learning (e.g., CNNs, autoencoders) is more suitable, such as for segmentation or disease prediction, and explain why.  \n",
    "3. For **each** approach (classic ML vs deep learning), state **one major challenge** (e.g., feature selection, overfitting, required data volume) in the context of neuroimaging.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Tasks\n",
    "\n",
    "- **Diagnosis/Prognosis:** Predict Alzheimer’s, schizophrenia, etc. from scans.  \n",
    "- **Cognitive decoding:** Identify stimulus/mental state from EEG/fMRI.  \n",
    "- **Biomarker discovery:** Locate predictive regions or networks. \n",
    "\n",
    "## Data Challenges & Preprocessing\n",
    "\n",
    "### - **High-dimensional, low-sample data** → overfitting risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem of underfitting and overfitting:\n",
    "\n",
    "\n",
    "\n",
    "* `Underfitting`: the model is too simple, the number of parameters $n$ is insufficient.\n",
    "\n",
    "* `Overfitting`: the model is too complex, there is an excessive number of parameters $n$.\n",
    "\n",
    "\n",
    "`What causes overfitting?`\n",
    "* excessive complexity of the pamameter space, extra degrees of freedom in the model $g(x, θ)$ are \"spent\" on overly accurate fitting to the training sample $X^l$;\n",
    "* overfitting is always there when there is a choice ($a$ from $A$) based on incomplete information (according to the final sample $X^l$).\n",
    "\n",
    "\n",
    "`How to detect overfitting?`\n",
    "* empirically, by dividing the sample into $\\text{train}$ and $\\text{test}$, and the correct answers should be known for $\\text{test}$.\n",
    "\n",
    "\n",
    "`You can't get rid of him. How to minimize it?`\n",
    "* minimize using `HoldOut`, `LOO` or `CV`, but be careful!!!\n",
    "* impose restrictions on $θ$ (regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation (CV)\n",
    "\n",
    "An `external criterion` evaluates the quality of \"out-of-training\", for example, by a hold-out control sample $X^k$:\n",
    "\\begin{equation}\n",
    "    Q_{\\mu}\\big(X^\\ell, X^k\\big) = Q\\big(\\mu\\big(X^\\ell\\big), X^k\\big).\n",
    "\\end{equation}\n",
    "\n",
    "Averaging `hold-out` estimates over a given $N$ - set of partitions $X^L = X_n^{\\ell} \\bigcup X_n^{k}, \\quad n = 1, \\ldots, N$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{CV}\\big(\\mu, X^L\\big) = \\frac{1}{\\vert N\\vert} \\sum_{n \\in N} Q_{\\mu}\\big(X_n^{\\ell}, X_n^{k}\\big).\n",
    "\\end{equation}\n",
    "\n",
    "Special cases are different ways of setting $N$.\n",
    "* A random set of partitions.\n",
    "* Complete cross-validation (CCV): $N$ is the set of all $C_{\\ell+k}^{k}$ partitions.\n",
    "\n",
    "`Disadvantage:` CCV estimation is computationally too complicated. Either small values of $k$ or combinatorial estimates of CCV are used.\n",
    "\n",
    "* `Sliding control` (Leave One Out CV): $~k=1$,\n",
    "\\begin{equation}\n",
    "    \\text{LOO}\\big(\\mu, X^L\\big) = \\frac{1}{L} \\sum_{n \\in N} Q_{\\mu}\\big(X^L \\backslash \\{x_i\\}, \\{x_i\\}\\big).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "`Disadvantage:` $\\text{LOO}$: resource intensive, high variance.\n",
    "\n",
    "\n",
    "* `Cross-checking` on $q$ blocks ($q$-fold CV): randomly splitting $X^L=X_1^{\\ell_1}\\bigcup\\ldots X_q^{\\ell_q}$ into $q$ blocks of (almost) equal length,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{CV}_q\\big(\\mu, X^L\\big) = \\frac{1}{q} \\sum_{n=1}^{q} Q_{\\mu}\\big(X^L \\backslash X_n^{\\ell_n}, ~X_n^{\\ell_n}\\big).\n",
    "\\end{equation}\n",
    "\n",
    "The `disadvantage` of $q$-fold CV:\n",
    "* the score depends significantly on the division into blocks;\n",
    "* Each object participates in the control only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3\n",
    ":class: dropdown checklist\n",
    "**Our methodological checklist**\n",
    "\n",
    "1. Strong regularization: L2/L1/ElasticNet, with hyperparameter tuning via nested CV.  \n",
    "2. Dimensionality reduction: PCA/PLS applied before the model (within a Pipeline to avoid data leakage).  \n",
    "3. Feature selection: L1-induced sparsity (SelectFromModel), mutual information, VarianceThreshold.  \n",
    "4. Simple models: linear methods (LR/SVM), shallow decision trees with limited depth.  \n",
    "5. Proper validation: stratified K-folds, nested CV, and a permutation test to confirm the model is not just “guessing”.  \n",
    "6. Learning curves: verify that adding more data improves performance rather than merely increasing model complexity.  \n",
    "7. Feature stability: resampling (bootstrap) and measuring the frequency of feature selection.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Noise/artifacts:** EEG (blinks, muscle), MRI (motion, scanner drift), fNIRS (physiological).  \n",
    "### - **Variability:** multi-site, multi-device, multimodal fusion issues.  \n",
    "\n",
    "## **Preprocessing examples:**  \n",
    "- EEG: filtering, artifact removal.  \n",
    "- fMRI: motion correction, normalization.  \n",
    "- fNIRS: light interference correction.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Traps (Often Forgotten, Even by Professionals)\n",
    "1.\tData leakage\n",
    "\n",
    "\t•\tTest data contaminates training (normalizing before split, ICA across full dataset).\n",
    "\n",
    "\t•\tSolution: Always split before preprocessing/feature selection.\n",
    "\n",
    "2.\tMultiple comparisons & statistical hypotheses\n",
    "\n",
    "\t•\tThousands of features → false positives.\n",
    "\n",
    "\t•\tSolution: Control via FDR, Bonferroni, permutation tests.\n",
    "\n",
    "3.\tCorrelation ≠ causation\n",
    "\n",
    "\t•\tBrain–behavior correlations may reflect confounds (motion, demographics, site).\n",
    "\n",
    "\t•\tSolution: Use covariates, stratified CV, domain adaptation.\n",
    "\n",
    "4.\tCircular analysis (“double dipping”)\n",
    "\n",
    "\t•\tSelecting voxels/features on same data used for testing.\n",
    "\n",
    "\t•\tSolution: Nested CV, independent validation sets.\n",
    "\n",
    "5.\tOverfitting\n",
    "\n",
    "\t•\tFew subjects vs. millions of features.\n",
    "\n",
    "\t•\tSolution: Report CV with subject-level separation, not trial-level.\n",
    "\n",
    "6.\tReporting bias\n",
    "\n",
    "\t•\tOnly reporting accuracy without variance or baseline.\n",
    "\t\n",
    "\t•\tSolution: Show chance levels, CI, label-shuffling controls.\n",
    "\n",
    "7.\tMisunderstanding the nature of data\n",
    "\n",
    "\t•\tNot all features or modalities are equivalent; missing modalities (e.g., absent fMRI scans, dropped EEG channels, incomplete behavioral data) require domain-aware strategies.\n",
    "\t\n",
    "\t•\tSolution: Before imputation or harmonization, understand why data are missing (technical failure, subject dropout, site heterogeneity).\n",
    "\n",
    "\t•\tSolution: Apply biologically and technically justified handling (e.g., modality-specific imputation, harmonization methods such as ComBat, or analysis restricted to common modalities).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
