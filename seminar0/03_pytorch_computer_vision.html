

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>03. PyTorch Computer Vision &#8212; Neuroimaging and Machine Learning for Biomedicine</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'seminar0/03_pytorch_computer_vision';</script>
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to the Neuroimaging and Machine Learning for Biomedicine coursebook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bio/intro_links.html">Links</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1AM67Sd37J0hIJ-kLyzAZRm8op_JcUQ9b?usp=sharing">Intro to shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="Seminar1_python_colab.html">How to work with Python</a></li>




<li class="toctree-l1"><a class="reference internal" href="Seminar1_ML.html">Intro to ML</a></li>

















<li class="toctree-l1"><a class="reference internal" href="00_pytorch_fundamentals.html">Pytorch fundamentals</a></li>
<li class="toctree-l1"><a class="reference external" href="https://drive.google.com/file/d/1q0XncKq2y_wHLXJaxhjfBCDGHABmzrjM/view?usp=sharing">Intro to computer vision</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Seminar 1. Working with EEG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../seminar1/seminar1-working-with-eeg.html">Seminar 1. EEG analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Seminars 2-3. Working with MRI</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../seminar2/seminar2_1_mri.html">Seminar 2.1. MRI data analysis, databases and tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../seminar2/seminar2_2_transform.html">Transformations</a></li>





<li class="toctree-l1"><a class="reference internal" href="../seminar2/seminar2_3_freesurfer.html">FREESURFER</a></li>


<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1i0f49198_Jb4fp-dw-8Dl46S8gKRyO8A?usp=sharing">Seminar 3.1. MRI classification on morphometry data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1fUSEvtxTgWYatJTBa1kHjKQU2vUW0V3v?usp=sharing">Seminar 3.2. MRI classification with 3D CNN</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/19yE-7yJDztLJHCvxFgMR6rGH-A3MFjsM?usp=sharing">Seminar 3.3. MRI segmentation with 3D U-net</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Seminar 4-6. Working with fMRI</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1V3pLq3B_AdMFBL3EVJD9VFU7GJ0PvAA5?usp=sharing">Seminar 5. Functional connectivity</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1elqHcFpcx1_QX4jhbNihM0whPgfJzBV3?usp=sharing">Seminar 6.1. Extract ICA components from fMRI data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1zFSdOD0HkhTi9r2l4t_pbW--FFgTK9fm?usp=sharing">Seminar 6.2. Deep Learning methods for fMRI data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Seminar 7. Interpretation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/1sJtU1uWezC-hEpDSkyhl0toioOt3F8FF?usp=sharing">Seminar 7. Interpretation of 3D CNNs for Brain MRI Data Classification</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/BIMAI-lab/NEUROML_course" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/BIMAI-lab/NEUROML_course/issues/new?title=Issue%20on%20page%20%2Fseminar0/03_pytorch_computer_vision.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/seminar0/03_pytorch_computer_vision.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>03. PyTorch Computer Vision</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-classification-problem">What is a classification problem?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-re-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-of-a-classification-neural-network">0. Architecture of a classification neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-libraries-in-pytorch">Computer vision libraries in PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-dataset">1. Getting a dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-shapes-of-a-computer-vision-model">1.1 Input and output shapes of a computer vision model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task-1">Task 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task-2">Task 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task3">Task3</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#which-type-of-problem-is-there">Which type of problem is there?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-our-data">1.2 Visualizing our data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-dataloader">2. Prepare DataLoader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task-4-which-shape-have-items-from-dataloader-print-it">Task 4: which shape have items from dataloader? print it</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-0-build-a-baseline-model">3. Model 0: Build a baseline model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task">Task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-loss-optimizer-and-evaluation-metrics">3.1 Setup loss, optimizer and evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-training-loop-and-training-a-model-on-batches-of-data">3.3 Creating a training loop and training a model on batches of data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-predictions-and-get-model-0-results">4. Make predictions and get Model 0 results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one">5. Setup device agnostic-code (for using a GPU if there is one)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1-building-a-better-model-with-non-linearity">6. Model 1: Building a better model with non-linearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">6.1 Setup loss, optimizer and evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#functionizing-training-and-test-loops">6.2 Functionizing training and test loops</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2-building-a-convolutional-neural-network-cnn">7. Model 2: Building a Convolutional Neural Network (CNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-model-should-i-use">What model should I use?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stepping-through-nn-conv2d">7.1 Stepping through <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stepping-through-nn-maxpool2d">7.2 Stepping through <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-a-loss-function-and-optimizer-for-model-2">7.3 Setup a loss function and optimizer for <code class="docutils literal notranslate"><span class="pre">model_2</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-model-2-using-our-training-and-test-functions">7.4 Training and testing <code class="docutils literal notranslate"><span class="pre">model_2</span></code> using our training and test functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-model-results-and-training-time">8. Compare model results and training time</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-speed-tradeoff">Performance-speed tradeoff</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-and-evaluate-random-predictions-with-best-model">9. Make and evaluate random predictions with best model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-confusion-matrix-for-further-prediction-evaluation">10. Making a confusion matrix for further prediction evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#save-and-load-best-performing-model">11. Save and load best performing model</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="pytorch-computer-vision">
<h1>03. PyTorch Computer Vision<a class="headerlink" href="#pytorch-computer-vision" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Computer_vision">Computer vision</a> is the art of teaching a computer to see.</p>
<p>For example, it could involve building a model to classify whether a photo is of a cat or a dog (<a class="reference external" href="https://developers.google.com/machine-learning/glossary#binary-classification">binary classification</a>).</p>
<p>Or whether a photo is of a cat, dog or chicken (<a class="reference external" href="https://developers.google.com/machine-learning/glossary#multi-class-classification">multi-class classification</a>).</p>
<p>Or identifying where a car appears in a video frame (<a class="reference external" href="https://en.wikipedia.org/wiki/Object_detection">object detection</a>).</p>
<p>Or figuring out where different objects in an image can be separated (<a class="reference external" href="https://arxiv.org/abs/1801.00868">panoptic segmentation</a>).</p>
<p><img alt="example computer vision problems" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png" />
<em>Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.</em></p>
<section id="what-is-a-classification-problem">
<h2>What is a classification problem?<a class="headerlink" href="#what-is-a-classification-problem" title="Permalink to this headline">#</a></h2>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_classification">classification problem</a> involves predicting whether something is one thing or another.</p>
<p>For example, you might want to:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem type</p></th>
<th class="head"><p>What is it?</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Binary classification</strong></p></td>
<td><p>Target can be one of two options, e.g. yes or no</p></td>
<td><p>Predict whether or not someone has heart disease based on their health parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multi-class classification</strong></p></td>
<td><p>Target can be one of more than two options</p></td>
<td><p>Decide whether a photo of is of food, a person or a dog.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Multi-label classification</strong></p></td>
<td><p>Target can be assigned more than one option</p></td>
<td><p>Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science &amp; philosohpy).</p></td>
</tr>
</tbody>
</table>
<div align="center">
<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-different-classification-problems.png" alt="various different classification in machine learning such as binary classification, multiclass classification and multilabel classification" width=900/>
</div>
</section>
<section id="what-we-re-going-to-cover">
<h2>What we’re going to cover<a class="headerlink" href="#what-we-re-going-to-cover" title="Permalink to this headline">#</a></h2>
<p>We’re going to apply the PyTorch Workflow we’ve been learning in the past couple of sections to computer vision.</p>
<p><img alt="a PyTorch workflow with a computer vision focus" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png" /></p>
<p>Specifically, we’re going to cover:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Topic</strong></p></th>
<th class="head"><p><strong>Contents</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>0. Computer vision libraries in PyTorch</strong></p></td>
<td><p>PyTorch has a bunch of built-in helpful computer vision libraries, let’s check them out.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>1. Load data</strong></p></td>
<td><p>To practice computer vision, we’ll start with some images of different pieces of clothing from <a class="reference external" href="https://github.com/zalandoresearch/fashion-mnist">FashionMNIST</a>.</p></td>
</tr>
<tr class="row-even"><td><p><strong>2. Prepare data</strong></p></td>
<td><p>We’ve got some images, let’s load them in with a <a class="reference external" href="https://pytorch.org/docs/stable/data.html">PyTorch <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a> so we can use them with our training loop.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>3. Model 0: Building a baseline model</strong></p></td>
<td><p>Here we’ll create a multi-class classification model to learn patterns in the data, we’ll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong>.</p></td>
</tr>
<tr class="row-even"><td><p><strong>4. Making predictions and evaluting model 0</strong></p></td>
<td><p>Let’s make some predictions with our baseline model and evaluate them.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>5. Setup device agnostic code for future models</strong></p></td>
<td><p>It’s best practice to write device-agnostic code, so let’s set it up.</p></td>
</tr>
<tr class="row-even"><td><p><strong>6. Model 1: Adding non-linearity</strong></p></td>
<td><p>Experimenting is a large part of machine learning, let’s try and improve upon our baseline model by adding non-linear layers.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>7. Model 2: Convolutional Neural Network (CNN)</strong></p></td>
<td><p>Time to get computer vision specific and introduce the powerful convolutional neural network architecture.</p></td>
</tr>
<tr class="row-even"><td><p><strong>8. Comparing our models</strong></p></td>
<td><p>We’ve built three different models, let’s compare them.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>9. Evaluating our best model</strong></p></td>
<td><p>Let’s make some predictons on random images and evaluate our best model.</p></td>
</tr>
<tr class="row-even"><td><p><strong>10. Making a confusion matrix</strong></p></td>
<td><p>A confusion matrix is a great way to evaluate a classification model, let’s see how we can make one.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>11. Saving and loading the best performing model</strong></p></td>
<td><p>Since we might want to use our model for later, let’s save it and make sure it loads back in correctly.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="architecture-of-a-classification-neural-network">
<h2>0. Architecture of a classification neural network<a class="headerlink" href="#architecture-of-a-classification-neural-network" title="Permalink to this headline">#</a></h2>
<p>Before we get into writing code, let’s look at the general architecture of a classification neural network.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Binary Classification</strong></p></th>
<th class="head"><p><strong>Multiclass classification</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Input layer shape</strong> (<code class="docutils literal notranslate"><span class="pre">in_features</span></code>)</p></td>
<td><p>Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction)</p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Hidden layer(s)</strong></p></td>
<td><p>Problem specific, minimum = 1, maximum = unlimited</p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-even"><td><p><strong>Neurons per hidden layer</strong></p></td>
<td><p>Problem specific, generally 10 to 512</p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Output layer shape</strong> (<code class="docutils literal notranslate"><span class="pre">out_features</span></code>)</p></td>
<td><p>1 (one class or the other)</p></td>
<td><p>1 per class (e.g. 3 for food, person or dog photo)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Hidden layer activation</strong></p></td>
<td><p>Usually <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a> (rectified linear unit) but <a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">can be many others</a></p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Output activation</strong></p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html"><code class="docutils literal notranslate"><span class="pre">torch.sigmoid</span></code></a> in PyTorch)</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html"><code class="docutils literal notranslate"><span class="pre">torch.softmax</span></code></a> in PyTorch)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Loss function</strong></p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression">Binary crossentropy</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss</span></code></a> in PyTorch)</p></td>
<td><p>Cross entropy (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a> in PyTorch)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Optimizer</strong></p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD</a> (stochastic gradient descent), <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a> (see <a class="reference external" href="https://pytorch.org/docs/stable/optim.html"><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code></a> for more options)</p></td>
<td><p>Same as binary classification</p></td>
</tr>
</tbody>
</table>
<p>Of course, this ingredient list of classification neural network components will vary depending on the problem you’re working on.</p>
<p>But it’s more than enough to get started.</p>
</section>
<section id="computer-vision-libraries-in-pytorch">
<h2>Computer vision libraries in PyTorch<a class="headerlink" href="#computer-vision-libraries-in-pytorch" title="Permalink to this headline">#</a></h2>
<p>Before we get started writing code, let’s talk about some PyTorch computer vision libraries you should be aware of.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch module</p></th>
<th class="head"><p>What does it do?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://pytorch.org/vision/stable/index.html"><code class="docutils literal notranslate"><span class="pre">torchvision</span></code></a></p></td>
<td><p>Contains datasets, model architectures and image transformations often used for computer vision problems.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://pytorch.org/vision/stable/datasets.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code></a></p></td>
<td><p>Here you’ll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains <a class="reference external" href="https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets">a series of base classes for making custom datasets</a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a></p></td>
<td><p>This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://pytorch.org/vision/stable/transforms.html"><code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code></a></p></td>
<td><p>Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code></a></p></td>
<td><p>Base dataset class for PyTorch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#module-torch.utils.data"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a></p></td>
<td><p>Creates a Python iterable over a dataset (created with <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>).</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p><strong>Note:</strong> The <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> classes aren’t only for computer vision in PyTorch, they are capable of dealing with many different types of data.</p>
</div></blockquote>
<p>Now we’ve covered some of the most important PyTorch computer vision libraries, let’s import the relevant dependencies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import PyTorch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Import torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>

<span class="c1"># Import matplotlib for visualization</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Check versions</span>
<span class="c1"># Note: your PyTorch version shouldn&#39;t be lower than 1.10.0 and torchvision version shouldn&#39;t be lower than 0.11</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="se">\n</span><span class="s2">torchvision version: </span><span class="si">{</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PyTorch version: 1.11.0
torchvision version: 0.12.0+cpu
</pre></div>
</div>
</div>
</div>
</section>
<section id="getting-a-dataset">
<h2>1. Getting a dataset<a class="headerlink" href="#getting-a-dataset" title="Permalink to this headline">#</a></h2>
<p>To begin working on a computer vision problem, let’s get a computer vision dataset.</p>
<p>We’re going to start with FashionMNIST.</p>
<p>MNIST stands for Modified National Institute of Standards and Technology.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">original MNIST dataset</a> contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.</p>
<p><a class="reference external" href="https://github.com/zalandoresearch/fashion-mnist">FashionMNIST</a>, made by Zalando Research, is a similar setup.</p>
<p>Except it contains grayscale images of 10 different kinds of clothing.</p>
<p><img alt="example image of FashionMNIST" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png" />
<em><code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code> contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it’s a multi-class classification problem.</em></p>
<p>Later, we’ll be building a computer vision neural network to identify the different styles of clothing in these images.</p>
<p>PyTorch has a bunch of common computer vision datasets stored in <code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code>.</p>
<p>Including FashionMNIST in <a class="reference external" href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.FashionMNIST()</span></code></a>.</p>
<p>To download it, we provide the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">root:</span> <span class="pre">str</span></code> - which folder do you want to download the data to?</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train:</span> <span class="pre">Bool</span></code> - do you want the training or test split?</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">download:</span> <span class="pre">Bool</span></code> - should the data be downloaded?</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transform:</span> <span class="pre">torchvision.transforms</span></code> - what transformations would you like to do on the data?</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_transform</span></code> - you can transform the targets (labels) if you like too.</p></li>
</ul>
<p>Many other datasets in <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> have these parameter options.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup training data</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="c1"># where to download data to?</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># get training data</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># download data if it doesn&#39;t exist on disk</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="c1"># images come as PIL format, we want to turn into Torch tensors</span>
    <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span> <span class="c1"># you can transform labels as well</span>
<span class="p">)</span>

<span class="c1"># Setup testing data</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># get test data</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check out the first sample of the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># See first training sample</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,
           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0039, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,
           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,
           0.0157, 0.0000, 0.0000, 0.0118],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,
           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0471, 0.0392, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,
           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,
           0.3020, 0.5098, 0.2824, 0.0588],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,
           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,
           0.5529, 0.3451, 0.6745, 0.2588],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,
           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,
           0.4824, 0.7686, 0.8980, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,
           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,
           0.8745, 0.9608, 0.6784, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,
           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,
           0.8627, 0.9529, 0.7922, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,
           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,
           0.8863, 0.7725, 0.8196, 0.2039],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,
           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,
           0.9608, 0.4667, 0.6549, 0.2196],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,
           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,
           0.8510, 0.8196, 0.3608, 0.0000],
          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,
           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,
           0.8549, 1.0000, 0.3020, 0.0000],
          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,
           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,
           0.8784, 0.9569, 0.6235, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,
           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,
           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,
           0.9137, 0.9333, 0.8431, 0.0000],
          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,
           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,
           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,
           0.8627, 0.9098, 0.9647, 0.0000],
          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,
           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,
           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,
           0.8706, 0.8941, 0.8824, 0.0000],
          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,
           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,
           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,
           0.8745, 0.8784, 0.8980, 0.1137],
          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,
           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,
           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,
           0.8627, 0.8667, 0.9020, 0.2627],
          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,
           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,
           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,
           0.7098, 0.8039, 0.8078, 0.4510],
          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,
           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,
           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,
           0.6549, 0.6941, 0.8235, 0.3608],
          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,
           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,
           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,
           0.7529, 0.8471, 0.6667, 0.0000],
          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,
           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,
           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,
           0.3882, 0.2275, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,
           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000]]]),
 9)
</pre></div>
</div>
</div>
</div>
<section id="input-and-output-shapes-of-a-computer-vision-model">
<h3>1.1 Input and output shapes of a computer vision model<a class="headerlink" href="#input-and-output-shapes-of-a-computer-vision-model" title="Permalink to this headline">#</a></h3>
<p>We’ve got a big tensor of values (the image) leading to a single value for the target (the label).</p>
<p>Let’s see the image shape.</p>
</section>
<section id="task-1">
<h3>Task 1<a class="headerlink" href="#task-1" title="Permalink to this headline">#</a></h3>
<p>See 10 training sample and check the image shape</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># What&#39;s the shape of the image?</span>
<span class="c1"># TODO </span>
</pre></div>
</div>
</div>
</div>
<p>The shape of the image tensor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">color_channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">]</span>
</pre></div>
</div>
<p>Having <code class="docutils literal notranslate"><span class="pre">color_channels=1</span></code> means the image is grayscale.</p>
<p><img alt="example input and output shapes of the fashionMNIST problem" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png" />
<em>Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.</em></p>
<p>If <code class="docutils literal notranslate"><span class="pre">color_channels=3</span></code>, the image comes in pixel values for red, green and blue (this is also known a the <a class="reference external" href="https://en.wikipedia.org/wiki/RGB_color_model">RGB color model</a>).</p>
<p>The order of our current tensor is often referred to as <code class="docutils literal notranslate"><span class="pre">CHW</span></code> (Color Channels, Height, Width).</p>
<p>There’s debate on whether images should be represented as <code class="docutils literal notranslate"><span class="pre">CHW</span></code> (color channels first) or <code class="docutils literal notranslate"><span class="pre">HWC</span></code> (color channels last).</p>
<blockquote>
<div><p><strong>Note:</strong> You’ll also see <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> and <code class="docutils literal notranslate"><span class="pre">NHWC</span></code> formats where <code class="docutils literal notranslate"><span class="pre">N</span></code> stands for <em>number of images</em>. For example if you have a <code class="docutils literal notranslate"><span class="pre">batch_size=32</span></code>, your tensor shape may be <code class="docutils literal notranslate"><span class="pre">[32,</span> <span class="pre">1,</span> <span class="pre">28,</span> <span class="pre">28]</span></code>. We’ll cover batch sizes later.</p>
</div></blockquote>
<p>PyTorch generally accepts <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> (channels first) as the default for many operators.</p>
<p>However, PyTorch also explains that <code class="docutils literal notranslate"><span class="pre">NHWC</span></code> (channels last) performs better and is <a class="reference external" href="https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice">considered best practice</a>.</p>
<p>For now, since our dataset and models are relatively small, this won’t make too much of a difference.</p>
<p>But keep it in mind for when you’re working on larger image datasets and using convolutional neural networks (we’ll see these later).</p>
<p>Let’s check out more shapes of our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># How many samples are there?</span>
<span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(60000, 60000)
</pre></div>
</div>
</div>
</div>
</section>
<section id="task-2">
<h3>Task 2<a class="headerlink" href="#task-2" title="Permalink to this headline">#</a></h3>
<p>Check the amount of test samples</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO</span>
</pre></div>
</div>
</div>
</div>
<p>What classes are there?</p>
<p>We can find these via the <code class="docutils literal notranslate"><span class="pre">.classes</span></code> attribute.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># See classes</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">classes</span>
<span class="n">class_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;T-shirt/top&#39;,
 &#39;Trouser&#39;,
 &#39;Pullover&#39;,
 &#39;Dress&#39;,
 &#39;Coat&#39;,
 &#39;Sandal&#39;,
 &#39;Shirt&#39;,
 &#39;Sneaker&#39;,
 &#39;Bag&#39;,
 &#39;Ankle boot&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="task3">
<h3>Task3<a class="headerlink" href="#task3" title="Permalink to this headline">#</a></h3>
<p>How many classes of clothes we have?</p>
<section id="which-type-of-problem-is-there">
<h4>Which type of problem is there?<a class="headerlink" href="#which-type-of-problem-is-there" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO print len of classes in dataset</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="visualizing-our-data">
<h3>1.2 Visualizing our data<a class="headerlink" href="#visualizing-our-data" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span> <span class="c1"># image shape is [1, 28, 28] (colour channels, height, width)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">label</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image shape: torch.Size([1, 28, 28])
</pre></div>
</div>
<img alt="../_images/289d4f57c5c6fe7bb746d7ab04ce4c83304d782e6f578e527d5fb19c66f21e0b.png" src="../_images/289d4f57c5c6fe7bb746d7ab04ce4c83304d782e6f578e527d5fb19c66f21e0b.png" />
</div>
</div>
<p>We can turn the image into grayscale using the <code class="docutils literal notranslate"><span class="pre">cmap</span></code> parameter of <code class="docutils literal notranslate"><span class="pre">plt.imshow()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/05c3a40cf496aba94e0c2dfa2c33c2058668fae65d797870aeaaf61fc25c09bc.png" src="../_images/05c3a40cf496aba94e0c2dfa2c33c2058668fae65d797870aeaaf61fc25c09bc.png" />
</div>
</div>
<p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p>
<p>Let’s view a few more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot more images</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">random_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">random_idx</span><span class="p">]</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/36e5dd6f6e3f0b82d905b53e06c61894f7e0418b40917618eaba4ef8a7ee56d6.png" src="../_images/36e5dd6f6e3f0b82d905b53e06c61894f7e0418b40917618eaba4ef8a7ee56d6.png" />
</div>
</div>
</section>
</section>
<section id="prepare-dataloader">
<h2>2. Prepare DataLoader<a class="headerlink" href="#prepare-dataloader" title="Permalink to this headline">#</a></h2>
<p>Now we’ve got a dataset ready to go.</p>
<p>The next step is to prepare it with a <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> or <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> for short.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> does what you think it might do.</p>
<p>It helps load data into a model.</p>
<p>For training and for inference.</p>
<p>It turns a large <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> into a Python iterable of smaller chunks.</p>
<p>These smaller chunks are called <strong>batches</strong> or <strong>mini-batches</strong> and can be set by the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> parameter.</p>
<p>Why do this?</p>
<p>Because it’s more computationally efficient.</p>
<p>In an ideal world you could do the forward pass and backward pass across all of your data at once.</p>
<p>But once you start using really large datasets, unless you’ve got infinite computing power, it’s easier to break them up into batches.</p>
<p>It also gives your model more opportunities to improve.</p>
<p>With <strong>mini-batches</strong> (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).</p>
<p>What’s a good batch size?</p>
<p><a class="reference external" href="https://twitter.com/ylecun/status/989610208497360896?s=20&amp;t=N96J_jotN--PYuJk2WcjMw">32 is a good place to start</a> for a fair amount of problems.</p>
<p>But since this is a value you can set (a <strong>hyperparameter</strong>) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).</p>
<p><img alt="an example of what a batched dataset looks like" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png" />
<em>Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.</em></p>
<p>Let’s create <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s for our training and test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Setup the batch size hyperparameter</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Turn datasets into iterables (batches)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="c1"># dataset to turn into iterable</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="c1"># how many samples per batch?</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># shuffle data every epoch?</span>
<span class="p">)</span>

<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># don&#39;t necessarily have to shuffle the testing data</span>
<span class="p">)</span>

<span class="c1"># Let&#39;s check out what we&#39;ve created</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataloaders: </span><span class="si">{</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of train dataloader: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s2"> batches of </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of test dataloader: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s2"> batches of </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x00000219CDA2A910&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x00000219CDA30460&gt;)
Length of train dataloader: 1875 batches of 32
Length of test dataloader: 313 batches of 32
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check out what&#39;s inside the training dataloader</span>
<span class="n">train_features_batch</span><span class="p">,</span> <span class="n">train_labels_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="task-4-which-shape-have-items-from-dataloader-print-it">
<h3>Task 4: which shape have items from dataloader? print it<a class="headerlink" href="#task-4-which-shape-have-items-from-dataloader-print-it" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO</span>
</pre></div>
</div>
</div>
</div>
<p>And we can see that the data remains unchanged by checking a single sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show a sample</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_features_batch</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_features_batch</span><span class="p">[</span><span class="n">random_idx</span><span class="p">],</span> <span class="n">train_labels_batch</span><span class="p">[</span><span class="n">random_idx</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;Off&quot;</span><span class="p">);</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image size: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">, label size: </span><span class="si">{</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image size: torch.Size([1, 28, 28])
Label: 6, label size: torch.Size([])
</pre></div>
</div>
<img alt="../_images/72d1dc2a5999d5e7acf3443e158cd16f6d6ce754ef018c6c4f51e8e6b78fe5c0.png" src="../_images/72d1dc2a5999d5e7acf3443e158cd16f6d6ce754ef018c6c4f51e8e6b78fe5c0.png" />
</div>
</div>
</section>
</section>
<section id="model-0-build-a-baseline-model">
<h2>3. Model 0: Build a baseline model<a class="headerlink" href="#model-0-build-a-baseline-model" title="Permalink to this headline">#</a></h2>
<p>Data loaded and prepared!</p>
<p>Time to build a <strong>baseline model</strong> by subclassing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p>
<p>A <strong>baseline model</strong> is one of the simplest models you can imagine.</p>
<p>You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.</p>
<p>Our baseline will consist of two <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code></a> layers.</p>
<p>Let’s create a model class that:</p>
<ol class="arabic simple">
<li><p>Subclasses <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> (almost all PyTorch models are subclasses of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>).</p></li>
<li><p>Creates 2 <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> layers in the constructor capable of handling the input and output shapes of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p></li>
<li><p>Defines a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method containing the forward pass computation of the model.</p></li>
<li><p>Instantiates the model class and sends it to the target <code class="docutils literal notranslate"><span class="pre">device</span></code>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Construct a model class that subclasses nn.Module</span>
<span class="k">class</span> <span class="nc">CircleModelV0</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># takes in 2 features (X), produces 5 features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># takes in 5 features, produces 1 feature (y)</span>
    
    <span class="c1"># 3. Define a forward method containing the forward pass computation</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Return the output of layer_2, a single feature, the same shape as y</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># computation goes through layer_1 first then the output of layer_1 goes through layer_2</span>

<span class="c1"># 4. Create an instance of the model and send it to target device</span>
<span class="n">model_0</span> <span class="o">=</span> <span class="n">CircleModelV0</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="ne">C</span>:\Users\ALEXAN~1\AppData\Local\Temp/ipykernel_8384/1631652932.py in &lt;module&gt;
<span class="g g-Whitespace">     </span><span class="mi">13</span> 
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="c1"># 4. Create an instance of the model and send it to target device</span>
<span class="ne">---&gt; </span><span class="mi">15</span> <span class="n">model_0</span> <span class="o">=</span> <span class="n">CircleModelV0</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="n">model_0</span>

<span class="ne">NameError</span>: name &#39;device&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>What’s going on here?</p>
<p>We’ve seen a few of these steps before.</p>
<p>The only major change is what’s happening between <code class="docutils literal notranslate"><span class="pre">self.layer_1</span></code> and <code class="docutils literal notranslate"><span class="pre">self.layer_2</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">self.layer_1</span></code> takes 2 input features <code class="docutils literal notranslate"><span class="pre">in_features=2</span></code> and produces 5 output features <code class="docutils literal notranslate"><span class="pre">out_features=5</span></code>.</p>
<p>This is known as having 5 <strong>hidden units</strong> or <strong>neurons</strong>.</p>
<p>This layer turns the input data from having 2 features to 5 features.</p>
<p>Why do this?</p>
<p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, <em>potentially</em> leading to better outputs.</p>
<p>I say potentially because sometimes it doesn’t work.</p>
<p>The number of hidden units you can use in neural network layers is a <strong>hyperparameter</strong> (a value you can set yourself) and there’s no set in stone value you have to use.</p>
<p>Generally more is better but there’s also such a thing as too much. The amount you choose will depend on your model type and dataset you’re working with.</p>
<p>Since our dataset is small and simple, we’ll keep it small.</p>
<p>The only rule with hidden units is that the next layer, in our case, <code class="docutils literal notranslate"><span class="pre">self.layer_2</span></code> has to take the same <code class="docutils literal notranslate"><span class="pre">in_features</span></code> as the previous layer <code class="docutils literal notranslate"><span class="pre">out_features</span></code>.</p>
<p>That’s why <code class="docutils literal notranslate"><span class="pre">self.layer_2</span></code> has <code class="docutils literal notranslate"><span class="pre">in_features=5</span></code>, it takes the <code class="docutils literal notranslate"><span class="pre">out_features=5</span></code> from <code class="docutils literal notranslate"><span class="pre">self.layer_1</span></code> and performs a linear computation on them, turning them into <code class="docutils literal notranslate"><span class="pre">out_features=1</span></code> (the same shape as <code class="docutils literal notranslate"><span class="pre">y</span></code>).</p>
<p><img alt="A visual example of what a classification neural network with linear activation looks like on the tensorflow playground" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-tensorflow-playground-linear-activation.png" />
<em>A visual example of what a similar classificiation neural network to the one we’ve just built looks like. Try create one of your own on the <a class="reference external" href="https://playground.tensorflow.org/">TensorFlow Playground website</a>.</em></p>
<p>You can also do the same as above using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> performs a forward pass computation of the input data through the layers in the order they appear.</p>
<p>Because we’re working with image data, we’re going to use a different layer to start things off.</p>
<p>And that’s the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code class="docutils literal notranslate"><span class="pre">nn.Flatten()</span></code></a> layer.</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Flatten()</span></code> compresses the dimensions of a tensor into a single vector.</p>
<p>This is easier to understand when you see it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a flatten layer</span>
<span class="n">flatten_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span> <span class="c1"># all nn modules function as a model (can do a forward pass)</span>

<span class="c1"># Get a single sample</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">train_features_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Flatten the sample</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">flatten_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># perform forward pass</span>

<span class="c1"># Print out what happened</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape before flattening: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [color_channels, height, width]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape after flattening: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [color_channels, height*width]&quot;</span><span class="p">)</span>

<span class="c1"># Try uncommenting below and see what happens</span>
<span class="c1"># print(x)</span>
<span class="c1"># print(output)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]
Shape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">nn.Flatten()</span></code> layer took our shape from <code class="docutils literal notranslate"><span class="pre">[color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code> to <code class="docutils literal notranslate"><span class="pre">[color_channels,</span> <span class="pre">height*width]</span></code>.</p>
<p>Why do this?</p>
<p>Because we’ve now turned our pixel data from height and width dimensions into one long <strong>feature vector</strong>.</p>
<p>And <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> layers like their inputs to be in the form of feature vectors.</p>
<p>Let’s create our first model using <code class="docutils literal notranslate"><span class="pre">nn.Flatten()</span></code> as the first layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="k">class</span> <span class="nc">FashionMNISTModelV0</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="c1"># neural networks like their inputs in vector form</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">),</span> <span class="c1"># in_features = number of features in a data sample (784 pixels)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>We’ve got a baseline model class we can use, now let’s instantiate a model.</p>
<p>We’ll need to set the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_shape=784</span></code> - this is how many features you’ve got going in the model, in our case, it’s one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_units=10</span></code> - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we’ll start with <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_shape=len(class_names)</span></code> - since we’re working with a multi-class classification problem, we need an output neuron per class in our dataset.</p></li>
</ul>
<p>Let’s create an instance of our model and send to the CPU for now (we’ll run a small test for running <code class="docutils literal notranslate"><span class="pre">model_0</span></code> on CPU vs. a similar model on GPU soon).</p>
<section id="task">
<h3>Task<a class="headerlink" href="#task" title="Permalink to this headline">#</a></h3>
<p>Add missing values</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Need to setup model with input parameters</span>
<span class="n">model_0</span> <span class="o">=</span> <span class="n">FashionMNISTModelV0</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">,</span> <span class="c1"># one for every pixel (28x28)</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="p">,</span> <span class="c1"># how many units in the hiden layer</span>
    <span class="n">output_shape</span><span class="o">=</span> <span class="c1"># one for every class</span>
<span class="p">)</span>
<span class="n">model_0</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="c1"># keep model on CPU to begin with</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FashionMNISTModelV0(
  (layer_stack): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=10, bias=True)
    (2): Linear(in_features=10, out_features=10, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="setup-loss-optimizer-and-evaluation-metrics">
<h3>3.1 Setup loss, optimizer and evaluation metrics<a class="headerlink" href="#setup-loss-optimizer-and-evaluation-metrics" title="Permalink to this headline">#</a></h3>
<p>We’ve setup a loss (also called a criterion or cost function) and optimizer.</p>
<p>But different problem types require different loss functions.</p>
<p>For example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.</p>
<p>And for a binary classification problem (like ours), you’ll often use <a class="reference external" href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">binary cross entropy</a> as the loss function.</p>
<p>However, the same optimizer function can often be used across different problem spaces.</p>
<p>For example, the stochastic gradient descent optimizer (SGD, <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code>) can be used for a range of problems, so can too the Adam optimizer (<code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code>).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Loss function/Optimizer</p></th>
<th class="head"><p>Problem type</p></th>
<th class="head"><p>PyTorch Code</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Stochastic Gradient Descent (SGD) optimizer</p></td>
<td><p>Classification, regression, many others.</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"><code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Adam Optimizer</p></td>
<td><p>Classification, regression, many others.</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p>Binary cross entropy loss</p></td>
<td><p>Binary classification</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELossWithLogits</span></code></a> or <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Cross entropy loss</p></td>
<td><p>Mutli-class classification</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p>Mean absolute error (MAE) or L1 Loss</p></td>
<td><p>Regression</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.L1Loss</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Mean squared error (MSE) or L2 Loss</p></td>
<td><p>Regression</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss"><code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code></a></p></td>
</tr>
</tbody>
</table>
<p><em>Table of various loss functions and optimizers, there are more but these some common ones you’ll see.</em></p>
<p>Since we’re working with a binary classification problem, let’s use a binary cross entropy loss function.</p>
<blockquote>
<div><p><strong>Note:</strong> Recall a <strong>loss function</strong> is what measures how <em>wrong</em> your model predictions are, the higher the loss, the worse your model.</p>
<p>Also, PyTorch documentation often refers to loss functions as “loss criterion” or “criterion”, these are all different ways of describing the same thing.</p>
</div></blockquote>
<p>PyTorch has two binary cross entropy implementations:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss()</span></code></a> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCEWithLogitsLoss()</span></code></a> - This is the same as above except it has a sigmoid layer (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html"><code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code></a>) built-in (we’ll see what this means soon).</p></li>
</ol>
<p>Which one should you use?</p>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">documentation for <code class="docutils literal notranslate"><span class="pre">torch.nn.BCEWithLogitsLoss()</span></code></a> states that it’s more numerically stable than using <code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss()</span></code> after a <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code> layer.</p>
<p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss()</span></code> but that is beyond the scope of this notebook.</p>
<p>Knowing this, let’s create a loss function and an optimizer.</p>
<p>For the optimizer we’ll use <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code> to optimize the model parameters with learning rate 0.1.</p>
<blockquote>
<div><p><strong>Note:</strong> There’s a <a class="reference external" href="https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4">discussion on the PyTorch forums about the use of <code class="docutils literal notranslate"><span class="pre">nn.BCELoss</span></code> vs. <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code></a>. It can be confusing at first but as with many things, it becomes easier with practice.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import accuracy metric</span>
<span class="kn">import</span> <span class="nn">torchmetrics</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">torchmetrics</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">(</span><span class="n">task</span> <span class="o">=</span> <span class="s1">&#39;multiclass&#39;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Setup loss function and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span> <span class="c1"># this is also called &quot;criterion&quot;/&quot;cost function&quot; in some places</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_0</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s also create an <strong>evaluation metric</strong>.</p>
<p>An evaluation metric can be used to offer another perspective on how your model is going.</p>
<p>If a loss function measures how <em>wrong</em> your model is, I like to think of evaluation metrics as measuring how <em>right</em> it is.</p>
<p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p>
<p>After all, when evaluating your models it’s good to look at things from multiple points of view.</p>
<p>There are several evaluation metrics that can be used for classification problems but let’s start out with <strong>accuracy</strong>.</p>
<p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p>
<p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p>
<p>Let’s write a function to do so.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate accuracy (a classification metric)</span>
<span class="k">def</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># torch.eq() calculates where two tensors are equal</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span> 
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="creating-a-training-loop-and-training-a-model-on-batches-of-data">
<h3>3.3 Creating a training loop and training a model on batches of data<a class="headerlink" href="#creating-a-training-loop-and-training-a-model-on-batches-of-data" title="Permalink to this headline">#</a></h3>
<p>Beautiful!</p>
<p>Looks like we’ve got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.</p>
<p>Let’s now create a training loop and a testing loop to train and evaluate our model.</p>
<p>We’ll be using the same steps as the previous notebook(s), though since our data is now in batch form, we’ll add another loop to loop through our data batches.</p>
<p>Our data batches are contained within our <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>s, <code class="docutils literal notranslate"><span class="pre">train_dataloader</span></code> and <code class="docutils literal notranslate"><span class="pre">test_dataloader</span></code> for the training and test data splits respectively.</p>
<p>A batch is <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span></code> samples of <code class="docutils literal notranslate"><span class="pre">X</span></code> (features) and <code class="docutils literal notranslate"><span class="pre">y</span></code> (labels), since we’re using <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE=32</span></code>, our batches have 32 samples of images and targets.</p>
<p>And since we’re computing on batches of data, our loss and evaluation metrics will be calculated <strong>per batch</strong> rather than across the whole dataset.</p>
<p>This means we’ll have to divide our loss and accuracy values by the number of batches in each dataset’s respective dataloader.</p>
<p>Let’s step through it:</p>
<ol class="arabic simple">
<li><p>Loop through epochs.</p></li>
<li><p>Loop through training batches, perform training steps, calculate the train loss <em>per batch</em>.</p></li>
<li><p>Loop through testing batches, perform testing steps, calculate the test loss <em>per batch</em>.</p></li>
<li><p>Print out what’s happening.</p></li>
<li><p>Time it all (for fun).</p></li>
</ol>
<p>A fair few steps but…</p>
<p>…if in doubt, code it out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import tqdm for progress bar</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Set the seed and start the timer</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_time_start_on_cpu</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>

<span class="c1"># Set the number of epochs (we&#39;ll keep this small for faster training times)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Create training and testing loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="se">\n</span><span class="s2">-------&quot;</span><span class="p">)</span>
    <span class="c1">### Training</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Add a loop to loop through training batches</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">model_0</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="c1"># 1. Forward pass</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># 2. Calculate loss (per batch)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span> <span class="c1"># accumulatively add up the loss per epoch</span>

        <span class="c1"># 3. Optimizer zero grad</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 4. Loss backward</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># 5. Optimizer step</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Print out how many samples have been seen</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">400</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Looked at </span><span class="si">{</span><span class="n">batch</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>

    <span class="c1"># Divide total train loss by length of train dataloader (average loss per batch per epoch)</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>

    <span class="c1">### Testing</span>
    <span class="c1"># Setup variables for accumulatively adding up loss and accuracy</span>
    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model_0</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
            <span class="c1"># 1. Forward pass</span>
            <span class="n">test_pred</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># 2. Calculate loss (accumatively)</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># accumulatively add up the loss per epoch</span>

            <span class="c1"># 3. Calculate accuracy (preds need to be same as y_true)</span>
            <span class="n">test_acc</span> <span class="o">+=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Calculations on test metrics need to happen inside torch.inference_mode()</span>
        <span class="c1"># Divide total test loss by length of test dataloader (per batch)</span>
        <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span>

        <span class="c1"># Divide total accuracy by length of test dataloader (per batch)</span>
        <span class="n">test_acc</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span>

    <span class="c1">## Print out what&#39;s happening</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> | Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Test acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Calculate training time</span>
<span class="n">train_time_end_on_cpu</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                                                                          | 0/3 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 33%|███████████████████████████████████████████▎                                                                                      | 1/3 [00:12&lt;00:24, 12.06s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train loss: 2.31847 | Test loss: 2.31906, Test acc: 10.85%

Epoch: 1
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 67%|██████████████████████████████████████████████████████████████████████████████████████▋                                           | 2/3 [00:24&lt;00:12, 12.22s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train loss: 2.31847 | Test loss: 2.31906, Test acc: 10.85%

Epoch: 2
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:36&lt;00:00, 12.10s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train loss: 2.31847 | Test loss: 2.31906, Test acc: 10.85%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>Nice! Looks like our baseline model did fairly well.</p>
<p>It didn’t take too long to train either, even just on the CPU, I wonder if it’ll speed up on the GPU?</p>
<p>Let’s write some code to evaluate our model.</p>
</section>
</section>
<section id="make-predictions-and-get-model-0-results">
<h2>4. Make predictions and get Model 0 results<a class="headerlink" href="#make-predictions-and-get-model-0-results" title="Permalink to this headline">#</a></h2>
<p>Since we’re going to be building a few models, it’s a good idea to write some code to evaluate them all in similar ways.</p>
<p>Namely, let’s create a function that takes in a trained model, a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>, a loss function and an accuracy function.</p>
<p>The function will use the model to make predictions on the data in the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> and then we can evaluate those predictions using the loss function and accuracy function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">accuracy_fn</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a dictionary containing the results of model predicting on data_loader.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.</span>
<span class="sd">        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.</span>
<span class="sd">        loss_fn (torch.nn.Module): The loss function of model.</span>
<span class="sd">        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (dict): Results of model making predictions on data_loader.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># Make predictions with the model</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># Accumulate the loss and accuracy values per batch</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)</span>

        <span class="c1"># Scale loss and acc to find the average loss/acc per batch</span>
        <span class="n">loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="c1"># only works when model was created with a class</span>
            <span class="s2">&quot;model_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;model_acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">}</span>

<span class="c1"># Calculate model 0 results on test dataset</span>
<span class="n">model_0_results</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_0</span><span class="p">,</span> <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
<span class="p">)</span>
<span class="n">model_0_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model_name&#39;: &#39;FashionMNISTModelV0&#39;,
 &#39;model_loss&#39;: 2.3190648555755615,
 &#39;model_acc&#39;: 10.852635782747603}
</pre></div>
</div>
</div>
</div>
<p>Looking good!</p>
<p>We can use this dictionary to compare the baseline model results to other models later on.</p>
</section>
<section id="setup-device-agnostic-code-for-using-a-gpu-if-there-is-one">
<h2>5. Setup device agnostic-code (for using a GPU if there is one)<a class="headerlink" href="#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one" title="Permalink to this headline">#</a></h2>
<p>We’ve seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.</p>
<blockquote>
<div><p><strong>Note:</strong> Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.</p>
</div></blockquote>
<p>Now let’s setup some <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#best-practices">device-agnostic code</a> for our models and data to run on GPU if it’s available.</p>
<p>If you’re running this notebook on Google Colab, and you don’t a GPU turned on yet, it’s now time to turn one on via <code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">-&gt;</span> <span class="pre">Change</span> <span class="pre">runtime</span> <span class="pre">type</span> <span class="pre">-&gt;</span> <span class="pre">Hardware</span> <span class="pre">accelerator</span> <span class="pre">-&gt;</span> <span class="pre">GPU</span></code>. If you do this, your runtime will likely reset and you’ll have to run all of the cells above by going <code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">-&gt;</span> <span class="pre">Run</span> <span class="pre">before</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup device agnostic code</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>Let’s build another model.</p>
</section>
<section id="model-1-building-a-better-model-with-non-linearity">
<h2>6. Model 1: Building a better model with non-linearity<a class="headerlink" href="#model-1-building-a-better-model-with-non-linearity" title="Permalink to this headline">#</a></h2>
<p>Seeing the data we’ve been working with, do you think it needs non-linear functions?</p>
<p>And remember, linear means straight and non-linear means non-straight.</p>
<p>Let’s find out.</p>
<p>We’ll do so by recreating a similar model to before, except this time we’ll put non-linear functions (<code class="docutils literal notranslate"><span class="pre">nn.ReLU()</span></code>) in between each linear layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a model with non-linear and linear layers</span>
<span class="k">class</span> <span class="nc">FashionMNISTModelV1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="c1"># flatten inputs into single vector</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That looks good.</p>
<p>Now let’s instantiate it with the same settings we used before.</p>
<p>We’ll need <code class="docutils literal notranslate"><span class="pre">input_shape=784</span></code> (equal to the number of features of our image data), <code class="docutils literal notranslate"><span class="pre">hidden_units=10</span></code> (starting small and the same as our baseline model) and <code class="docutils literal notranslate"><span class="pre">output_shape=len(class_names)</span></code> (one output unit per class).</p>
<blockquote>
<div><p><strong>Note:</strong> Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="n">FashionMNISTModelV1</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="c1"># number of input features</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">output_shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span> <span class="c1"># number of output classes desired</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># send model to GPU if it&#39;s available</span>
<span class="nb">next</span><span class="p">(</span><span class="n">model_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span> <span class="c1"># check model device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>device(type=&#39;cuda&#39;, index=0)
</pre></div>
</div>
</div>
</div>
<section id="id1">
<h3>6.1 Setup loss, optimizer and evaluation metrics<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>As usual, we’ll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we’ll stick with accuracy for now).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">accuracy_fn</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                            <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="functionizing-training-and-test-loops">
<h3>6.2 Functionizing training and test loops<a class="headerlink" href="#functionizing-training-and-test-loops" title="Permalink to this headline">#</a></h3>
<p>So far we’ve been writing train and test loops over and over.</p>
<p>Let’s write them again but this time we’ll put them in functions so they can be called again and again.</p>
<p>And because we’re using device-agnostic code now, we’ll be sure to call <code class="docutils literal notranslate"><span class="pre">.to(device)</span></code> on our feature (<code class="docutils literal notranslate"><span class="pre">X</span></code>) and target (<code class="docutils literal notranslate"><span class="pre">y</span></code>) tensors.</p>
<p>For the training loop we’ll create a function called <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> which takes in a model, a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> a loss function and an optimizer.</p>
<p>The testing loop will be similar but it’ll be called <code class="docutils literal notranslate"><span class="pre">test_step()</span></code> and it’ll take in a model, a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>, a loss function and an evaluation function.</p>
<blockquote>
<div><p><strong>Note:</strong> Since these are functions, you can customize them in any way you like. What we’re making here can be considered barebones training and testing functions for our specific classification use case.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
               <span class="n">accuracy_fn</span><span class="p">,</span>
               <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="c1"># Send data to GPU</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 1. Forward pass</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># 2. Calculate loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                 <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># Go from logits -&gt; pred labels</span>

        <span class="c1"># 3. Optimizer zero grad</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 4. Loss backward</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># 5. Optimizer step</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Calculate loss and accuracy per epoch and print out what&#39;s happening</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="n">train_acc</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> | Train accuracy: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
              <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
              <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
              <span class="n">accuracy_fn</span><span class="p">,</span>
              <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># put model in eval mode</span>
    <span class="c1"># Turn on inference context manager</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># Send data to GPU</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># 1. Forward pass</span>
            <span class="n">test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># 2. Calculate loss and accuracy</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">test_acc</span> <span class="o">+=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Go from logits -&gt; pred labels</span>
            <span class="p">)</span>

        <span class="c1"># Adjust metrics and print out</span>
        <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2"> | Test accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Woohoo!</p>
<p>Now we’ve got some functions for training and testing our model, let’s run them.</p>
<p>We’ll do so inside another loop for each epoch.</p>
<p>That way for each epoch we’re going a training and a testing step.</p>
<blockquote>
<div><p><strong>Note:</strong> You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.</p>
</div></blockquote>
<p>Let’s also time things to see how long our code takes to run on the GPU.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Measure time</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span>
<span class="n">train_time_start_on_gpu</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="se">\n</span><span class="s2">---------&quot;</span><span class="p">)</span>
    <span class="n">train_step</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
    <span class="p">)</span>
    <span class="n">test_step</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
    <span class="p">)</span>

<span class="n">train_time_end_on_gpu</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>
<span class="n">total_train_time_model_1</span> <span class="o">=</span> <span class="n">print_train_time</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">train_time_start_on_gpu</span><span class="p">,</span>
                                            <span class="n">end</span><span class="o">=</span><span class="n">train_time_end_on_gpu</span><span class="p">,</span>
                                            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                                                                          | 0/3 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0
---------
Train loss: 1.09199 | Train accuracy: 61.34%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 33%|███████████████████████████████████████████▎                                                                                      | 1/3 [00:12&lt;00:24, 12.33s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 0.95636 | Test accuracy: 65.00%

Epoch: 1
---------
Train loss: 0.78101 | Train accuracy: 71.93%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 67%|██████████████████████████████████████████████████████████████████████████████████████▋                                           | 2/3 [00:24&lt;00:12, 12.15s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 0.72227 | Test accuracy: 73.91%

Epoch: 2
---------
Train loss: 0.67027 | Train accuracy: 75.94%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:36&lt;00:00, 12.21s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 0.68500 | Test accuracy: 75.02%

Train time on cuda: 36.633 seconds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>Excellent!</p>
<p>Our model trained but the training time took longer?</p>
<blockquote>
<div><p><strong>Note:</strong> The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you’re using. Read on for a more explained answer.</p>
</div></blockquote>
<blockquote>
<div><p><strong>Question:</strong> “I used a a GPU but my model didn’t train faster, why might that be?”</p>
<p><strong>Answer:</strong> Well, one reason could be because your dataset and model are both so small (like the dataset and model we’re working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.</p>
<p>There’s a small bottleneck between copying data from the CPU memory (default) to the GPU memory.</p>
<p>So for smaller models and datasets, the CPU might actually be the optimal place to compute on.</p>
<p>But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.</p>
<p>However, this is largely dependant on the hardware you’re using. With practice, you will get used to where the best place to train your models is.</p>
</div></blockquote>
<p>Let’s evaluate our trained <code class="docutils literal notranslate"><span class="pre">model_1</span></code> using our <code class="docutils literal notranslate"><span class="pre">eval_model()</span></code> function and see how it went.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Note: This will error due to `eval_model()` not using device agnostic code</span>
<span class="n">model_1_results</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
    <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">)</span>
<span class="n">model_1_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">55</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># Note: This will error due to `eval_model()` not using device agnostic code</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">model_1_results</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>     <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>     <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">model_1_results</span>

<span class="nn">Cell In[48], line 22,</span> in <span class="ni">eval_model</span><span class="nt">(model, data_loader, loss_fn, accuracy_fn)</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span>     <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span>         <span class="c1"># Make predictions with the model</span>
<span class="ne">---&gt; </span><span class="mi">22</span>         <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">24</span>         <span class="c1"># Accumulate the loss and accuracy values per batch</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span>         <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/module.py:1518,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1518</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/module.py:1527,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1524</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1525</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1526</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1527</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1529</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1530</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">Cell In[50], line 14,</span> in <span class="ni">FashionMNISTModelV1.forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">14</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/module.py:1518,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1518</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/module.py:1527,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1524</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1525</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1526</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1527</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1529</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1530</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/container.py:215,</span> in <span class="ni">Sequential.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">213</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">214</span>     <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">215</span>         <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">216</span>     <span class="k">return</span> <span class="nb">input</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/module.py:1518,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1516</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1517</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1518</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/module.py:1527,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1522</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1523</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1524</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1525</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1526</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1527</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1529</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1530</span>     <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File /opt/miniconda-latest/lib/python3.8/site-packages/torch/nn/modules/linear.py:114,</span> in <span class="ni">Linear.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">113</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">114</span>     <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="ne">RuntimeError</span>: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
</pre></div>
</div>
</div>
</div>
<p>Oh no!</p>
<p>It looks like our <code class="docutils literal notranslate"><span class="pre">eval_model()</span></code> function errors out with:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Expected</span> <span class="pre">all</span> <span class="pre">tensors</span> <span class="pre">to</span> <span class="pre">be</span> <span class="pre">on</span> <span class="pre">the</span> <span class="pre">same</span> <span class="pre">device,</span> <span class="pre">but</span> <span class="pre">found</span> <span class="pre">at</span> <span class="pre">least</span> <span class="pre">two</span> <span class="pre">devices,</span> <span class="pre">cuda:0</span> <span class="pre">and</span> <span class="pre">cpu!</span> <span class="pre">(when</span> <span class="pre">checking</span> <span class="pre">argument</span> <span class="pre">for</span> <span class="pre">argument</span> <span class="pre">mat1</span> <span class="pre">in</span> <span class="pre">method</span> <span class="pre">wrapper_addmm)</span></code></p>
</div></blockquote>
<p>It’s because we’ve setup our data and model to use device-agnostic code but not our evaluation function.</p>
<p>How about we fix that by passing a target <code class="docutils literal notranslate"><span class="pre">device</span></code> parameter to our <code class="docutils literal notranslate"><span class="pre">eval_model()</span></code> function?</p>
<p>Then we’ll try calculating the results again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Move values to device</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">accuracy_fn</span><span class="p">,</span>
               <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates a given model on a given dataset.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.</span>
<span class="sd">        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.</span>
<span class="sd">        loss_fn (torch.nn.Module): The loss function of model.</span>
<span class="sd">        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.</span>
<span class="sd">        device (str, optional): Target device to compute on. Defaults to device.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (dict): Results of model making predictions on data_loader.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># Send data to the target device</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Scale loss and acc</span>
        <span class="n">loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="c1"># only works when model was created with a class</span>
            <span class="s2">&quot;model_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s2">&quot;model_acc&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">}</span>

<span class="c1"># Calculate model 1 results with device-agnostic code</span>
<span class="n">model_1_results</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span> <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span>
<span class="p">)</span>
<span class="n">model_1_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model_name&#39;: &#39;FashionMNISTModelV1&#39;,
 &#39;model_loss&#39;: 0.6850008964538574,
 &#39;model_acc&#39;: 75.01996805111821}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check baseline results</span>
<span class="n">model_0_results</span>
</pre></div>
</div>
</div>
</div>
<p>Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.</p>
<p>That’s a thing to note in machine learning, sometimes the thing you thought should work doesn’t.</p>
<p>And then the thing you thought might not work does.</p>
<p>It’s part science, part art.</p>
<p>From the looks of things, it seems like our model is <strong>overfitting</strong> on the training data.</p>
<p>Overfitting means our model is learning the training data well but those patterns aren’t generalizing to the testing data.</p>
<p>Two of the main to fix overfitting include:</p>
<ol class="arabic simple">
<li><p>Using a smaller or different model (some models fit certain kinds of data better than others).</p></li>
<li><p>Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).</p></li>
</ol>
<p>There are more, but I’m going to leave that as a challenge for you to explore.</p>
<p>Try searching online, “ways to prevent overfitting in machine learning” and see what comes up.</p>
<p>In the meantime, let’s take a look at number 1: using a different model.</p>
</section>
</section>
<section id="model-2-building-a-convolutional-neural-network-cnn">
<h2>7. Model 2: Building a Convolutional Neural Network (CNN)<a class="headerlink" href="#model-2-building-a-convolutional-neural-network-cnn" title="Permalink to this headline">#</a></h2>
<p>Alright, time to step things up a notch.</p>
<p>It’s time to create a <a class="reference external" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network</a> (CNN or ConvNet).</p>
<p>CNN’s are known for their capabilities to find patterns in visual data.</p>
<p>And since we’re dealing with visual data, let’s see if using a CNN model can improve upon our baseline.</p>
<p>The CNN model we’re going to be using is known as TinyVGG from the <a class="reference external" href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> website.</p>
<p>It follows the typical structure of a convolutional neural network:</p>
<p><code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">[Convolutional</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">activation</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">pooling</span> <span class="pre">layer]</span> <span class="pre">-&gt;</span> <span class="pre">Output</span> <span class="pre">layer</span></code></p>
<p>Where the contents of <code class="docutils literal notranslate"><span class="pre">[Convolutional</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">activation</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">pooling</span> <span class="pre">layer]</span></code> can be upscaled and repeated multiple times, depending on requirements.</p>
<section id="what-model-should-i-use">
<h3>What model should I use?<a class="headerlink" href="#what-model-should-i-use" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p><strong>Question:</strong> Wait, you say CNN’s are good for images, are there any other model types I should be aware of?</p>
</div></blockquote>
<p>Good question.</p>
<p>This table is a good general guide for which model to use (though there are exceptions).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Problem type</strong></p></th>
<th class="head"><p><strong>Model to use (generally)</strong></p></th>
<th class="head"><p><strong>Code example</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Structured data (Excel spreadsheets, row and column data)</p></td>
<td><p>Gradient boosted models, Random Forests, XGBoost</p></td>
<td><p><a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"><code class="docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a>, <a class="reference external" href="https://xgboost.readthedocs.io/en/stable/">XGBoost library</a></p></td>
</tr>
<tr class="row-odd"><td><p>Unstructured data (images, audio, language)</p></td>
<td><p>Convolutional Neural Networks, Transformers</p></td>
<td><p><a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a>, <a class="reference external" href="https://huggingface.co/docs/transformers/index">HuggingFace Transformers</a></p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p><strong>Note:</strong> The table above is only for reference, the model you end up using will be highly dependant on the problem you’re working on and the constraints you have (amount of data, latency requirements).</p>
</div></blockquote>
<p>Enough talking about models, let’s now build a CNN that replicates the model on the <a class="reference external" href="https://poloclub.github.io/cnn-explainer/">CNN Explainer website</a>.</p>
<p><img alt="TinyVGG architecture, as setup by CNN explainer website" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-cnn-explainer-model.png" /></p>
<p>To do so, we’ll leverage the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"><code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code></a> layers from <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a convolutional neural network</span>
<span class="k">class</span> <span class="nc">FashionMNISTModelV2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Model architecture copying TinyVGG from:</span>
<span class="sd">    https://poloclub.github.io/cnn-explainer/</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># how big is the square that&#39;s going over the image?</span>
                      <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># default</span>
                      <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="c1"># options = &quot;valid&quot; (no padding) or &quot;same&quot; (output has same shape as input) or int for specific number</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                      <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                         <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># default stride value is same as kernel_size</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="c1"># Where did this in_features shape come from?</span>
            <span class="c1"># It&#39;s because each layer of our network compresses and changes the shape of our inputs data.</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_2</span> <span class="o">=</span> <span class="n">FashionMNISTModelV2</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">output_shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FashionMNISTModelV2(
  (block_1): Sequential(
    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block_2): Sequential(
    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=490, out_features=10, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<p>Nice!</p>
<p>Our biggest model yet!</p>
<p>What we’ve done is a common practice in machine learning.</p>
<p>Find a model architecture somewhere and replicate it with code.</p>
</section>
<section id="stepping-through-nn-conv2d">
<h3>7.1 Stepping through <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code><a class="headerlink" href="#stepping-through-nn-conv2d" title="Permalink to this headline">#</a></h3>
<p>We could start using our model above and see what happens but let’s first step through the two new layers we’ve added:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a>, also known as a convolutional layer.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"><code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code></a>, also known as a max pooling layer.</p></li>
</ul>
<blockquote>
<div><p><strong>Question:</strong> What does the “2d” in <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> stand for?</p>
<p>The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there’s color channel dimension but each of the color channel dimensions have two dimensions too: height and width.</p>
<p>For other dimensional data (such as 1D for text or 3D for 3D objects) there’s also <code class="docutils literal notranslate"><span class="pre">nn.Conv1d()</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Conv3d()</span></code>.</p>
</div></blockquote>
<p>To test the layers out, let’s create some toy data just like the data used on CNN Explainer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create sample batch of random numbers with same size as image batch</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span> <span class="c1"># [batch_size, color_channels, height, width]</span>
<span class="n">test_image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># get a single image for testing</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image batch shape: </span><span class="si">{</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, color_channels, height, width]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Single image shape: </span><span class="si">{</span><span class="n">test_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [color_channels, height, width]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Single image pixel values:</span><span class="se">\n</span><span class="si">{</span><span class="n">test_image</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]
Single image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]
Single image pixel values:
tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],
         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],
         ...,
         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],
         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],
         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],

        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],
         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],
         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],
         ...,
         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],
         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],
         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],

        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],
         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],
         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],
         ...,
         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],
         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],
         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])
</pre></div>
</div>
</div>
</div>
<p>Let’s create an example <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> with various parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">in_channels</span></code> (int) - Number of channels in the input image.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_channels</span></code> (int) - Number of channels produced by the convolution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> (int or tuple) - Size of the convolving kernel/filter.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding</span></code> (int, tuple, str) - Padding added to all four sides of input. Default: 0.</p></li>
</ul>
<p><img alt="example of going through the different parameters of a Conv2d layer" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif" /></p>
<p><em>Example of what happens when you change the hyperparameters of a <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> layer.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create a convolutional layer with same dimensions as TinyVGG</span>
<span class="c1"># (try changing any of the parameters and see what happens)</span>
<span class="n">conv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                       <span class="n">out_channels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                       <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                       <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># also try using &quot;valid&quot; or &quot;same&quot; here</span>

<span class="c1"># Pass the data through the convolutional layer</span>
<span class="n">conv_layer</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span> <span class="c1"># Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],
         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],
         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],
         ...,
         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],
         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],
         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],

        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],
         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],
         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],
         ...,
         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],
         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],
         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],

        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],
         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],
         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],
         ...,
         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],
         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],
         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],

        ...,

        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],
         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],
         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],
         ...,
         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],
         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],
         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],

        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],
         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],
         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],
         ...,
         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],
         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],
         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],

        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],
         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],
         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],
         ...,
         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],
         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],
         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],
       grad_fn=&lt;SqueezeBackward1&gt;)
</pre></div>
</div>
</div>
</div>
<p>If we try to pass a single image in, we get a shape mismatch error:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Expected</span> <span class="pre">4-dimensional</span> <span class="pre">input</span> <span class="pre">for</span> <span class="pre">4-dimensional</span> <span class="pre">weight</span> <span class="pre">[10,</span> <span class="pre">3,</span> <span class="pre">3,</span> <span class="pre">3],</span> <span class="pre">but</span> <span class="pre">got</span> <span class="pre">3-dimensional</span> <span class="pre">input</span> <span class="pre">of</span> <span class="pre">size</span> <span class="pre">[3,</span> <span class="pre">64,</span> <span class="pre">64]</span> <span class="pre">instead</span></code></p>
<p><strong>Note:</strong> If you’re running PyTorch 1.11.0+, this error won’t occur.</p>
</div></blockquote>
<p>This is because our <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> layer expects a 4-dimensional tensor as input with size <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">C,</span> <span class="pre">H,</span> <span class="pre">W)</span></code> or <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code>.</p>
<p>Right now our single image <code class="docutils literal notranslate"><span class="pre">test_image</span></code> only has a shape of <code class="docutils literal notranslate"><span class="pre">[color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code> or <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">64,</span> <span class="pre">64]</span></code>.</p>
<p>We can fix this for a single image using <code class="docutils literal notranslate"><span class="pre">test_image.unsqueeze(dim=0)</span></code> to add an extra dimension for <code class="docutils literal notranslate"><span class="pre">N</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add extra dimension to test image</span>
<span class="n">test_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 3, 64, 64])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pass test image with extra dimension through conv_layer</span>
<span class="n">conv_layer</span><span class="p">(</span><span class="n">test_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 10, 62, 62])
</pre></div>
</div>
</div>
</div>
<p>Hmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on <a class="reference external" href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a>), we get different channel sizes as well as different pixel sizes.</p>
<p>What if we changed the values of <code class="docutils literal notranslate"><span class="pre">conv_layer</span></code>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Create a new conv_layer with different values (try setting these to whatever you like)</span>
<span class="n">conv_layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># same number of color channels as our input image</span>
                         <span class="n">out_channels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                         <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="c1"># kernel is usually a square so a tuple also works</span>
                         <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                         <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Pass single image through new conv_layer_2 (this calls nn.Conv2d()&#39;s forward() method on the input)</span>
<span class="n">conv_layer_2</span><span class="p">(</span><span class="n">test_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 10, 30, 30])
</pre></div>
</div>
</div>
</div>
<p>Woah, we get another shape change.</p>
<p>Now our image is of shape <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">10,</span> <span class="pre">30,</span> <span class="pre">30]</span></code> (it will be different if you use different values) or <code class="docutils literal notranslate"><span class="pre">[batch_size=1,</span> <span class="pre">color_channels=10,</span> <span class="pre">height=30,</span> <span class="pre">width=30]</span></code>.</p>
<p>What’s going on here?</p>
<p>Behind the scenes, our <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> is compressing the information stored in the image.</p>
<p>It does this by performing operations on the input (our test image) against its internal parameters.</p>
<p>The goal of this is similar to all of the other neural networks we’ve been building.</p>
<p>Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.</p>
<p>The only difference is <em>how</em> the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
<p>If we check out our <code class="docutils literal notranslate"><span class="pre">conv_layer_2.state_dict()</span></code> we’ll find a similar weight and bias setup as we’ve seen before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check out the conv_layer_2 internal parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">conv_layer_2</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;weight&#39;, tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],
          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],
          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],
          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],
          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],

         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],
          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],
          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],
          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],
          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],

         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],
          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],
          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],
          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],
          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],


        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],
          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],
          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],
          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],
          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],

         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],
          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],
          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],
          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],
          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],

         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],
          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],
          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],
          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],
          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],


        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],
          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],
          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],
          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],
          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],

         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],
          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],
          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],
          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],
          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],

         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],
          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],
          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],
          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],
          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],


        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],
          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],
          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],
          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],
          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],

         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],
          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],
          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],
          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],
          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],

         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],
          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],
          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],
          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],
          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],


        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],
          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],
          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],
          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],
          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],

         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],
          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],
          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],
          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],
          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],

         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],
          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],
          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],
          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],
          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],


        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],
          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],
          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],
          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],
          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],

         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],
          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],
          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],
          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],
          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],

         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],
          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],
          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],
          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],
          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],


        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],
          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],
          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],
          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],
          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],

         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],
          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],
          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],
          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],
          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],

         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],
          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],
          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],
          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],
          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],


        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],
          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],
          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],
          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],
          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],

         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],
          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],
          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],
          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],
          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],

         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],
          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],
          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],
          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],
          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],


        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],
          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],
          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],
          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],
          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],

         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],
          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],
          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],
          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],
          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],

         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],
          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],
          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],
          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],
          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],


        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],
          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],
          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],
          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],
          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],

         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],
          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],
          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],
          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],
          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],

         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],
          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],
          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],
          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],
          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), (&#39;bias&#39;, tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,
        -0.0797,  0.1121]))])
</pre></div>
</div>
</div>
</div>
<p>Look at that! A bunch of random numbers for a weight and bias tensor.</p>
<p>The shapes of these are manipulated by the inputs we passed to <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> when we set it up.</p>
<p>Let’s check them out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get shapes of weight and bias tensors within conv_layer_2</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;conv_layer_2 weight shape: </span><span class="se">\n</span><span class="si">{</span><span class="n">conv_layer_2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">conv_layer_2 bias shape: </span><span class="se">\n</span><span class="si">{</span><span class="n">conv_layer_2</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [out_channels=10]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>conv_layer_2 weight shape: 
torch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]

conv_layer_2 bias shape: 
torch.Size([10]) -&gt; [out_channels=10]
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Question:</strong> What should we set the parameters of our <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> layers?</p>
<p>That’s a good one. But similar to many other things in machine learning, the values of these aren’t set in stone (and recall, because these values are ones we can set ourselves, they’re referred to as “<strong>hyperparameters</strong>”).</p>
<p>The best way to find out is to try out different values and see how they effect your model’s performance.</p>
<p>Or even better, find a working example on a problem similar to yours (like we’ve done with TinyVGG) and copy it.</p>
</div></blockquote>
<p>We’re working with a different of layer here to what we’ve seen before.</p>
<p>But the premise remains the same: start with random numbers and update them to better represent the data.</p>
</section>
<section id="stepping-through-nn-maxpool2d">
<h3>7.2 Stepping through <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code><a class="headerlink" href="#stepping-through-nn-maxpool2d" title="Permalink to this headline">#</a></h3>
<p>Now let’s check out what happens when we move data through <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out original image shape without and with unsqueezed dimension</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test image original shape: </span><span class="si">{</span><span class="n">test_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test image with unsqueezed dimension: </span><span class="si">{</span><span class="n">test_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create a sample nn.MaxPoo2d() layer</span>
<span class="n">max_pool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Pass data through just the conv_layer</span>
<span class="n">test_image_through_conv</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">test_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape after going through conv_layer(): </span><span class="si">{</span><span class="n">test_image_through_conv</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Pass data through the max pool layer</span>
<span class="n">test_image_through_conv_and_max_pool</span> <span class="o">=</span> <span class="n">max_pool_layer</span><span class="p">(</span><span class="n">test_image_through_conv</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape after going through conv_layer() and max_pool_layer(): </span><span class="si">{</span><span class="n">test_image_through_conv_and_max_pool</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test image original shape: torch.Size([3, 64, 64])
Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])
Shape after going through conv_layer(): torch.Size([1, 10, 62, 62])
Shape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])
</pre></div>
</div>
</div>
</div>
<p>Notice the change in the shapes of what’s happening in and out of a <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code> layer.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> of the <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code> layer will effects the size of the output shape.</p>
<p>In our case, the shape halves from a <code class="docutils literal notranslate"><span class="pre">62x62</span></code> image to <code class="docutils literal notranslate"><span class="pre">31x31</span></code> image.</p>
<p>Let’s see this work with a smaller tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Create a random tensor with a similiar number of dimensions to our images</span>
<span class="n">random_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">random_tensor</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random tensor shape: </span><span class="si">{</span><span class="n">random_tensor</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create a max pool layer</span>
<span class="n">max_pool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># see what happens when you change the kernel_size value</span>

<span class="c1"># Pass the random tensor through the max pool layer</span>
<span class="n">max_pool_tensor</span> <span class="o">=</span> <span class="n">max_pool_layer</span><span class="p">(</span><span class="n">random_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Max pool tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">max_pool_tensor</span><span class="si">}</span><span class="s2"> &lt;- this is the maximum value from random_tensor&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max pool tensor shape: </span><span class="si">{</span><span class="n">max_pool_tensor</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random tensor:
tensor([[[[0.3367, 0.1288],
          [0.2345, 0.2303]]]])
Random tensor shape: torch.Size([1, 1, 2, 2])

Max pool tensor:
tensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor
Max pool tensor shape: torch.Size([1, 1, 1, 1])
</pre></div>
</div>
</div>
</div>
<p>Notice the final two dimensions between <code class="docutils literal notranslate"><span class="pre">random_tensor</span></code> and <code class="docutils literal notranslate"><span class="pre">max_pool_tensor</span></code>, they go from <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">2]</span></code> to <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">1]</span></code>.</p>
<p>In essence, they get halved.</p>
<p>And the change would be different for different values of <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> for <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code>.</p>
<p>Also notice the value leftover in <code class="docutils literal notranslate"><span class="pre">max_pool_tensor</span></code> is the <strong>maximum</strong> value from <code class="docutils literal notranslate"><span class="pre">random_tensor</span></code>.</p>
<p>What’s happening here?</p>
<p>This is another important piece of the puzzle of neural networks.</p>
<p>Essentially, <strong>every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space</strong>.</p>
<p>In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being <em>smaller</em> in size than the original values.</p>
<p>From an artificial intelligence perspective, you could consider the whole goal of a neural network to <em>compress</em> information.</p>
<p><img alt="each layer of a neural network compresses the original input data into a smaller representation that is (hopefully) capable of making predictions on future input data" src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png" /></p>
<p>This means, that from the point of view of a neural network, intelligence is compression.</p>
<p>This is the idea of the use of a <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code> layer: take the maximum value from a portion of a tensor and disregard the rest.</p>
<p>In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.</p>
<p>It is the same story for a <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> layer.</p>
<p>Except instead of just taking the maximum, the <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> performs a convolutional operation on the data (see this in action on the <a class="reference external" href="https://poloclub.github.io/cnn-explainer/">CNN Explainer webpage</a>).</p>
<blockquote>
<div><p><strong>Exercise:</strong> What do you think the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html"><code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d()</span></code></a> layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.</p>
</div></blockquote>
<blockquote>
<div><p><strong>Extra-curriculum:</strong> Lookup “most common convolutional neural networks”, what architectures do you find? Are any of them contained within the <a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a> library? What do you think you could do with these?</p>
</div></blockquote>
</section>
<section id="setup-a-loss-function-and-optimizer-for-model-2">
<h3>7.3 Setup a loss function and optimizer for <code class="docutils literal notranslate"><span class="pre">model_2</span></code><a class="headerlink" href="#setup-a-loss-function-and-optimizer-for-model-2" title="Permalink to this headline">#</a></h3>
<p>We’ve stepped through the layers in our first CNN enough.</p>
<p>But remember, if something still isn’t clear, try starting small.</p>
<p>Pick a single layer of a model, pass some data through it and see what happens.</p>
<p>Now it’s time to move forward and get to training!</p>
<p>Let’s setup a loss function and an optimizer.</p>
<p>We’ll use the functions as before, <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss()</span></code> as the loss function (since we’re working with multi-class classification data).</p>
<p>And <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code> as the optimizer to optimize <code class="docutils literal notranslate"><span class="pre">model_2.parameters()</span></code> with a learning rate of <code class="docutils literal notranslate"><span class="pre">0.1</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup loss and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                             <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-and-testing-model-2-using-our-training-and-test-functions">
<h3>7.4 Training and testing <code class="docutils literal notranslate"><span class="pre">model_2</span></code> using our training and test functions<a class="headerlink" href="#training-and-testing-model-2-using-our-training-and-test-functions" title="Permalink to this headline">#</a></h3>
<p>Loss and optimizer ready!</p>
<p>Time to train and test.</p>
<p>We’ll use our <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">test_step()</span></code> functions we created before.</p>
<p>We’ll also measure the time to compare it to our other models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Measure time</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span>
<span class="n">train_time_start_model_2</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>

<span class="c1"># Train and test model</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="se">\n</span><span class="s2">---------&quot;</span><span class="p">)</span>
    <span class="n">train_step</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_2</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">test_step</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_2</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>

<span class="n">train_time_end_model_2</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>
<span class="n">total_train_time_model_2</span> <span class="o">=</span> <span class="n">print_train_time</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">train_time_start_model_2</span><span class="p">,</span>
                                           <span class="n">end</span><span class="o">=</span><span class="n">train_time_end_model_2</span><span class="p">,</span>
                                           <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                                                                          | 0/3 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0
---------
Train loss: 0.59091 | Train accuracy: 78.48%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 33%|███████████████████████████████████████████▎                                                                                      | 1/3 [00:15&lt;00:31, 15.65s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 0.40978 | Test accuracy: 85.33%

Epoch: 1
---------
Train loss: 0.36067 | Train accuracy: 87.01%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 67%|██████████████████████████████████████████████████████████████████████████████████████▋                                           | 2/3 [00:30&lt;00:15, 15.30s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 0.35329 | Test accuracy: 87.05%

Epoch: 2
---------
Train loss: 0.32304 | Train accuracy: 88.36%
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:46&lt;00:00, 15.36s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test loss: 0.32050 | Test accuracy: 88.30%

Train time on cuda: 46.080 seconds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>Woah! Looks like the convolutional and max pooling layers helped improve performance a little.</p>
<p>Let’s evaluate <code class="docutils literal notranslate"><span class="pre">model_2</span></code>’s results with our <code class="docutils literal notranslate"><span class="pre">eval_model()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get model_2 results</span>
<span class="n">model_2_results</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_2</span><span class="p">,</span>
    <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
<span class="p">)</span>
<span class="n">model_2_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model_name&#39;: &#39;FashionMNISTModelV2&#39;,
 &#39;model_loss&#39;: 0.3205042779445648,
 &#39;model_acc&#39;: 88.29872204472844}
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="compare-model-results-and-training-time">
<h2>8. Compare model results and training time<a class="headerlink" href="#compare-model-results-and-training-time" title="Permalink to this headline">#</a></h2>
<p>We’ve trained three different models.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model_0</span></code> - our baseline model with two <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_1</span></code> - the same setup as our baseline model except with <code class="docutils literal notranslate"><span class="pre">nn.ReLU()</span></code> layers in between the <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_2</span></code> - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.</p></li>
</ol>
<p>This is a regular practice in machine learning.</p>
<p>Building multiple models and performing multiple training experiments to see which performs best.</p>
<p>Let’s combine our model results dictionaries into a DataFrame and find out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">compare_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">model_0_results</span><span class="p">,</span> <span class="n">model_1_results</span><span class="p">,</span> <span class="n">model_2_results</span><span class="p">])</span>
<span class="n">compare_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>model_loss</th>
      <th>model_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>FashionMNISTModelV0</td>
      <td>2.319065</td>
      <td>10.852636</td>
    </tr>
    <tr>
      <th>1</th>
      <td>FashionMNISTModelV1</td>
      <td>0.685001</td>
      <td>75.019968</td>
    </tr>
    <tr>
      <th>2</th>
      <td>FashionMNISTModelV2</td>
      <td>0.320504</td>
      <td>88.298722</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Nice!</p>
<p>We can add the training time values too.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add training times to results comparison</span>
<span class="n">compare_results</span><span class="p">[</span><span class="s2">&quot;training_time&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">total_train_time_model_0</span><span class="p">,</span>
                                    <span class="n">total_train_time_model_1</span><span class="p">,</span>
                                    <span class="n">total_train_time_model_2</span><span class="p">]</span>
<span class="n">compare_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_name</th>
      <th>model_loss</th>
      <th>model_acc</th>
      <th>training_time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>FashionMNISTModelV0</td>
      <td>2.319065</td>
      <td>10.852636</td>
      <td>37.862587</td>
    </tr>
    <tr>
      <th>1</th>
      <td>FashionMNISTModelV1</td>
      <td>0.685001</td>
      <td>75.019968</td>
      <td>36.632944</td>
    </tr>
    <tr>
      <th>2</th>
      <td>FashionMNISTModelV2</td>
      <td>0.320504</td>
      <td>88.298722</td>
      <td>46.079818</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It looks like our CNN (<code class="docutils literal notranslate"><span class="pre">FashionMNISTModelV2</span></code>) model performed the best (lowest loss, highest accuracy) but had the longest training time.</p>
<p>And our baseline model (<code class="docutils literal notranslate"><span class="pre">FashionMNISTModelV0</span></code>) performed better than <code class="docutils literal notranslate"><span class="pre">model_1</span></code> (<code class="docutils literal notranslate"><span class="pre">FashionMNISTModelV1</span></code>).</p>
<section id="performance-speed-tradeoff">
<h3>Performance-speed tradeoff<a class="headerlink" href="#performance-speed-tradeoff" title="Permalink to this headline">#</a></h3>
<p>Something to be aware of in machine learning is the <strong>performance-speed</strong> tradeoff.</p>
<p>Generally, you get better performance out of a larger, more complex model (like we did with <code class="docutils literal notranslate"><span class="pre">model_2</span></code>).</p>
<p>However, this performance increase often comes at a sacrifice of training speed and inference speed.</p>
<blockquote>
<div><p><strong>Note:</strong> The training times you get will be very dependant on the hardware you use.</p>
<p>Generally, the more CPU cores you have, the faster your models will train on CPU. And similar for GPUs.</p>
<p>Newer hardware (in terms of age) will also often train models faster due to incorporating technology advances.</p>
</div></blockquote>
<p>How about we get visual?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize our model results</span>
<span class="n">compare_results</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;model_name&quot;</span><span class="p">)[</span><span class="s2">&quot;model_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;barh&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;accuracy (%)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8a45533d42b86ae8c012f3a81eb73206a74295fd51d76efc4f771dca9fd02722.png" src="../_images/8a45533d42b86ae8c012f3a81eb73206a74295fd51d76efc4f771dca9fd02722.png" />
</div>
</div>
</section>
</section>
<section id="make-and-evaluate-random-predictions-with-best-model">
<h2>9. Make and evaluate random predictions with best model<a class="headerlink" href="#make-and-evaluate-random-predictions-with-best-model" title="Permalink to this headline">#</a></h2>
<p>Alright, we’ve compared our models to each other, let’s further evaluate our best performing model, <code class="docutils literal notranslate"><span class="pre">model_2</span></code>.</p>
<p>To do so, let’s create a function <code class="docutils literal notranslate"><span class="pre">make_predictions()</span></code> where we can pass the model and some data for it to predict on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">pred_probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="c1"># Prepare sample</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># Add an extra dimension and send sample to device</span>

            <span class="c1"># Forward pass (model outputs raw logit)</span>
            <span class="n">pred_logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

            <span class="c1"># Get prediction probability (logit -&gt; prediction probability)</span>
            <span class="n">pred_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_logit</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># note: perform softmax on the &quot;logits&quot; dimension, not &quot;batch&quot; dimension (in this case we have a batch size of 1, so can perform on dim=0)</span>

            <span class="c1"># Get pred_prob off GPU for further calculations</span>
            <span class="n">pred_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_prob</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

    <span class="c1"># Stack the pred_probs to turn list into a tensor</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pred_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">test_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">test_data</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">test_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="n">test_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="c1"># View the first test sample shape and label</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test sample image shape: </span><span class="si">{</span><span class="n">test_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">Test sample label: </span><span class="si">{</span><span class="n">test_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">test_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test sample image shape: torch.Size([1, 28, 28])
Test sample label: 5 (Sandal)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions on test samples with model 2</span>
<span class="n">pred_probs</span><span class="o">=</span> <span class="n">make_predictions</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_2</span><span class="p">,</span>
                             <span class="n">data</span><span class="o">=</span><span class="n">test_samples</span><span class="p">)</span>

<span class="c1"># View first two prediction probabilities list</span>
<span class="n">pred_probs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[5.2742e-08, 3.2726e-09, 2.7340e-08, 1.0877e-08, 4.7134e-09, 9.9989e-01,
         1.5191e-07, 3.5352e-06, 4.0862e-05, 6.1391e-05],
        [2.3750e-02, 7.8195e-01, 3.1331e-04, 5.5720e-02, 8.4735e-02, 3.3254e-05,
         5.3178e-02, 1.8690e-04, 7.2145e-05, 5.8120e-05]])
</pre></div>
</div>
</div>
</div>
<p>And now we can use our <code class="docutils literal notranslate"><span class="pre">make_predictions()</span></code> function to predict on <code class="docutils literal notranslate"><span class="pre">test_samples</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions on test samples with model 2</span>
<span class="n">pred_probs</span><span class="o">=</span> <span class="n">make_predictions</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_2</span><span class="p">,</span>
                             <span class="n">data</span><span class="o">=</span><span class="n">test_samples</span><span class="p">)</span>

<span class="c1"># View first two prediction probabilities list</span>
<span class="n">pred_probs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[2.4012e-07, 6.5406e-08, 4.8069e-08, 2.1070e-07, 1.4175e-07, 9.9992e-01,
         2.1711e-07, 1.6177e-05, 3.7849e-05, 2.7548e-05],
        [1.5646e-02, 8.9752e-01, 3.6928e-04, 6.7402e-02, 1.2920e-02, 4.9539e-05,
         5.6485e-03, 1.9456e-04, 2.0808e-04, 3.7861e-05]])
</pre></div>
</div>
</div>
</div>
<p>Excellent!</p>
<p>And now we can go from prediction probabilities to prediction labels by taking the <code class="docutils literal notranslate"><span class="pre">torch.argmax()</span></code> of the output of the <code class="docutils literal notranslate"><span class="pre">torch.softmax()</span></code> activation function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn the prediction probabilities into prediction labels by taking the argmax()</span>
<span class="n">pred_classes</span> <span class="o">=</span> <span class="n">pred_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pred_classes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Are our predictions in the same form as our test labels?</span>
<span class="n">test_labels</span><span class="p">,</span> <span class="n">pred_classes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))
</pre></div>
</div>
</div>
</div>
<p>Now our predicted classes are in the same format as our test labels, we can compare.</p>
<p>Since we’re dealing with image data, let’s stay true to the data explorer’s motto.</p>
<p>“Visualize, visualize, visualize!”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot predictions</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">nrows</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">ncols</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_samples</span><span class="p">):</span>
  <span class="c1"># Create a subplot</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

  <span class="c1"># Plot the target image</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>

  <span class="c1"># Find the prediction label (in text form, e.g. &quot;Sandal&quot;)</span>
  <span class="n">pred_label</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">[</span><span class="n">pred_classes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>

  <span class="c1"># Get the truth label (in text form, e.g. &quot;T-shirt&quot;)</span>
  <span class="n">truth_label</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">[</span><span class="n">test_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>

  <span class="c1"># Create the title text of the plot</span>
  <span class="n">title_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Pred: </span><span class="si">{</span><span class="n">pred_label</span><span class="si">}</span><span class="s2"> | Truth: </span><span class="si">{</span><span class="n">truth_label</span><span class="si">}</span><span class="s2">&quot;</span>

  <span class="c1"># Check for equality and change title colour accordingly</span>
  <span class="k">if</span> <span class="n">pred_label</span> <span class="o">==</span> <span class="n">truth_label</span><span class="p">:</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title_text</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span> <span class="c1"># green text if correct</span>
  <span class="k">else</span><span class="p">:</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title_text</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="c1"># red text if wrong</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d2bfd08a427d0bc663af186e2d22e13e1220e666f45f35b1e970e404585770c1.png" src="../_images/d2bfd08a427d0bc663af186e2d22e13e1220e666f45f35b1e970e404585770c1.png" />
</div>
</div>
<p>Well, well, well, doesn’t that look good!</p>
<p>Not bad for a couple dozen lines of PyTorch code!</p>
</section>
<section id="making-a-confusion-matrix-for-further-prediction-evaluation">
<h2>10. Making a confusion matrix for further prediction evaluation<a class="headerlink" href="#making-a-confusion-matrix-for-further-prediction-evaluation" title="Permalink to this headline">#</a></h2>
<p>There are many <a class="reference external" href="https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics">different evaluation metrics</a> we can use for classification problems.</p>
<p>One of the most visual is a <a class="reference external" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">confusion matrix</a>.</p>
<p>A confusion matrix shows you where your classification model got confused between predicitons and true labels.</p>
<p>To make a confusion matrix, we’ll go through three steps:</p>
<ol class="arabic simple">
<li><p>Make predictions with our trained model, <code class="docutils literal notranslate"><span class="pre">model_2</span></code> (a confusion matrix compares predictions to true labels).</p></li>
<li><p>Make a confusion matrix using <a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code class="docutils literal notranslate"><span class="pre">torchmetrics.ConfusionMatrix</span></code></a>.</p></li>
<li><p>Plot the confusion matrix using <a class="reference external" href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code class="docutils literal notranslate"><span class="pre">mlxtend.plotting.plot_confusion_matrix()</span></code></a>.</p></li>
</ol>
<p>Let’s start by making predictions with our trained model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import tqdm for progress bar</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 1. Make predictions with trained model</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">model_2</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Making predictions&quot;</span><span class="p">):</span>
    <span class="c1"># Send data and targets to target device</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Do the forward pass</span>
    <span class="n">y_logit</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_logit</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># note: perform softmax on the &quot;logits&quot; dimension, not &quot;batch&quot; dimension (in this case we have a batch size of 32, so can perform on dim=1)</span>
    <span class="c1"># Put predictions on CPU for evaluation</span>
    <span class="n">y_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
<span class="c1"># Concatenate list of predictions into a tensor</span>
<span class="n">y_pred_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">y_preds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Making predictions: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:01&lt;00:00, 189.31it/s]
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>Now we’ve got predictions, let’s go through steps 2 &amp; 3:
2. Make a confusion matrix using <a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code class="docutils literal notranslate"><span class="pre">torchmetrics.ConfusionMatrix</span></code></a>.
3. Plot the confusion matrix using <a class="reference external" href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code class="docutils literal notranslate"><span class="pre">mlxtend.plotting.plot_confusion_matrix()</span></code></a>.</p>
<p>First we’ll need to make sure we’ve got <code class="docutils literal notranslate"><span class="pre">torchmetrics</span></code> and <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> installed (these two libraries will help us make and visual a confusion matrix).</p>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, the default version of <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> installed is 0.14.0 (as of March 2022), however, for the parameters of the <code class="docutils literal notranslate"><span class="pre">plot_confusion_matrix()</span></code> function we’d like use, we need 0.19.0 or higher.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># See if torchmetrics exists, if not, install it
try:
    import torchmetrics, mlxtend
    print(f&quot;mlxtend version: {mlxtend.__version__}&quot;)
    assert int(mlxtend.__version__.split(&quot;.&quot;)[1]) &gt;= 19, &quot;mlxtend verison should be 0.19.0 or higher&quot;
except:
    !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you&#39;re using Google Colab, this may require restarting the runtime
    import torchmetrics, mlxtend
    print(f&quot;mlxtend version: {mlxtend.__version__}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Yellow">WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv</span>
mlxtend version: 0.23.0
</pre></div>
</div>
</div>
</div>
<p>To plot the confusion matrix, we need to make sure we’ve got and <a class="reference external" href="http://rasbt.github.io/mlxtend/"><code class="docutils literal notranslate"><span class="pre">mlxtend</span></code></a> version of 0.19.0 or higher.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import mlxtend upgraded version</span>
<span class="kn">import</span> <span class="nn">mlxtend</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mlxtend</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">mlxtend</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">19</span> <span class="c1"># should be version 0.19.0 or higher</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.23.0
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torchmetrics</span></code> and <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> installed, let’s make a confusion matrix!</p>
<p>First we’ll create a <code class="docutils literal notranslate"><span class="pre">torchmetrics.ConfusionMatrix</span></code> instance telling it how many classes we’re dealing with by setting <code class="docutils literal notranslate"><span class="pre">num_classes=len(class_names)</span></code>.</p>
<p>Then we’ll create a confusion matrix (in tensor format) by passing our instance our model’s predictions (<code class="docutils literal notranslate"><span class="pre">preds=y_pred_tensor</span></code>) and targets (<code class="docutils literal notranslate"><span class="pre">target=test_data.targets</span></code>).</p>
<p>Finally we can plot our confision matrix using the <code class="docutils literal notranslate"><span class="pre">plot_confusion_matrix()</span></code> function from <code class="docutils literal notranslate"><span class="pre">mlxtend.plotting</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrix</span>
<span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_confusion_matrix</span>

<span class="c1"># 2. Setup confusion matrix instance and compare predictions to targets</span>
<span class="n">confmat</span> <span class="o">=</span> <span class="n">ConfusionMatrix</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span> <span class="n">task</span><span class="o">=</span><span class="s1">&#39;multiclass&#39;</span><span class="p">)</span>
<span class="n">confmat_tensor</span> <span class="o">=</span> <span class="n">confmat</span><span class="p">(</span><span class="n">preds</span><span class="o">=</span><span class="n">y_pred_tensor</span><span class="p">,</span>
                         <span class="n">target</span><span class="o">=</span><span class="n">test_data</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>

<span class="c1"># 3. Plot the confusion matrix</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span>
    <span class="n">conf_mat</span><span class="o">=</span><span class="n">confmat_tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="c1"># matplotlib likes working with NumPy</span>
    <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="c1"># turn the row and column labels into class names</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e159af74075d7b47953ea3f7a2d5432094c80d2547d1d22edfc354858f1cf49b.png" src="../_images/e159af74075d7b47953ea3f7a2d5432094c80d2547d1d22edfc354858f1cf49b.png" />
</div>
</div>
<p>Woah! Doesn’t that look good?</p>
<p>We can see our model does fairly well since most of the dark squares are down the diagonal from top left to bottom right (and ideal model will have only values in these squares and 0 everywhere else).</p>
<p>The model gets most “confused” on classes that are similar, for example predicting “Pullover” for images that are actually labelled “Shirt”.</p>
<p>And the same for predicting “Shirt” for classes that are actually labelled “T-shirt/top”.</p>
<p>This kind of information is often more helpful than a single accuracy metric because it tells use <em>where</em> a model is getting things wrong.</p>
<p>It also hints at <em>why</em> the model may be getting certain things wrong.</p>
<p>It’s understandable the model sometimes predicts “Shirt” for images labelled “T-shirt/top”.</p>
<p>We can use this kind of information to further inspect our models and data to see how it could be improved.</p>
<blockquote>
<div><p><strong>Exercise:</strong> Use the trained <code class="docutils literal notranslate"><span class="pre">model_2</span></code> to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should’ve been. After visualing these predictions do you think it’s more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a “Shirt” label is too close to “T-shirt/top”)?</p>
</div></blockquote>
</section>
<section id="save-and-load-best-performing-model">
<h2>11. Save and load best performing model<a class="headerlink" href="#save-and-load-best-performing-model" title="Permalink to this headline">#</a></h2>
<p>Let’s finish this section off by saving and loading in our best performing model.</p>
<p>Recall from <a class="reference external" href="https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model">notebook 01</a> we can save and load a PyTorch model using a combination of:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.save</span></code> - a function to save a whole PyTorch model or a model’s <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.load</span></code> - a function to load in a saved PyTorch object.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.Module.load_state_dict()</span></code> - a function to load a saved <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> into an existing model instance.</p></li>
</ul>
<p>You can see more of these three in the <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch saving and loading models documentation</a>.</p>
<p>For now, let’s save our <code class="docutils literal notranslate"><span class="pre">model_2</span></code>’s <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> then load it back in and evaluate it to make sure the save and load went correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Create models directory (if it doesn&#39;t already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir</span>
<span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;models&quot;</span><span class="p">)</span>
<span class="n">MODEL_PATH</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># create parent directories if needed</span>
                 <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># if models directory already exists, don&#39;t error</span>
<span class="p">)</span>

<span class="c1"># Create model save path</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;03_pytorch_computer_vision_model_2.pth&quot;</span>
<span class="n">MODEL_SAVE_PATH</span> <span class="o">=</span> <span class="n">MODEL_PATH</span> <span class="o">/</span> <span class="n">MODEL_NAME</span>

<span class="c1"># Save the model state dict</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saving model to: </span><span class="si">{</span><span class="n">MODEL_SAVE_PATH</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">obj</span><span class="o">=</span><span class="n">model_2</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="c1"># only saving the state_dict() only saves the learned parameters</span>
           <span class="n">f</span><span class="o">=</span><span class="n">MODEL_SAVE_PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving model to: models/03_pytorch_computer_vision_model_2.pth
</pre></div>
</div>
</div>
</div>
<p>Now we’ve got a saved model <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> we can load it back in using a combination of <code class="docutils literal notranslate"><span class="pre">load_state_dict()</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p>
<p>Since we’re using <code class="docutils literal notranslate"><span class="pre">load_state_dict()</span></code>, we’ll need to create a new instance of <code class="docutils literal notranslate"><span class="pre">FashionMNISTModelV2()</span></code> with the same input parameters as our saved model <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())</span>
<span class="c1"># Note: loading model will error if the shapes here aren&#39;t the same as the saved version</span>
<span class="n">loaded_model_2</span> <span class="o">=</span> <span class="n">FashionMNISTModelV2</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># try changing this to 128 and seeing what happens</span>
                                    <span class="n">output_shape</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Load in the saved state_dict()</span>
<span class="n">loaded_model_2</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">MODEL_SAVE_PATH</span><span class="p">))</span>

<span class="c1"># Send model to GPU</span>
<span class="n">loaded_model_2</span> <span class="o">=</span> <span class="n">loaded_model_2</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And now we’ve got a loaded model we can evaluate it with <code class="docutils literal notranslate"><span class="pre">eval_model()</span></code> to make sure its parameters work similarly to <code class="docutils literal notranslate"><span class="pre">model_2</span></code> prior to saving.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate loaded model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">loaded_model_2_results</span> <span class="o">=</span> <span class="n">eval_model</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">loaded_model_2</span><span class="p">,</span>
    <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
<span class="p">)</span>

<span class="n">loaded_model_2_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model_name&#39;: &#39;FashionMNISTModelV2&#39;,
 &#39;model_loss&#39;: 0.3205042779445648,
 &#39;model_acc&#39;: 88.29872204472844}
</pre></div>
</div>
</div>
</div>
<p>Do these results look the same as <code class="docutils literal notranslate"><span class="pre">model_2_results</span></code>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_2_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;model_name&#39;: &#39;FashionMNISTModelV2&#39;,
 &#39;model_loss&#39;: 0.3205042779445648,
 &#39;model_acc&#39;: 88.29872204472844}
</pre></div>
</div>
</div>
</div>
<p>We can find out if two tensors are close to each other using <code class="docutils literal notranslate"><span class="pre">torch.isclose()</span></code> and passing in a tolerance level of closeness via the parameters <code class="docutils literal notranslate"><span class="pre">atol</span></code> (absolute tolerance) and <code class="docutils literal notranslate"><span class="pre">rtol</span></code> (relative tolerance).</p>
<p>If our model’s results are close, the output of <code class="docutils literal notranslate"><span class="pre">torch.isclose()</span></code> should be true.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./seminar0"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-classification-problem">What is a classification problem?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-re-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-of-a-classification-neural-network">0. Architecture of a classification neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-libraries-in-pytorch">Computer vision libraries in PyTorch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-dataset">1. Getting a dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-shapes-of-a-computer-vision-model">1.1 Input and output shapes of a computer vision model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task-1">Task 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task-2">Task 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task3">Task3</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#which-type-of-problem-is-there">Which type of problem is there?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-our-data">1.2 Visualizing our data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-dataloader">2. Prepare DataLoader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task-4-which-shape-have-items-from-dataloader-print-it">Task 4: which shape have items from dataloader? print it</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-0-build-a-baseline-model">3. Model 0: Build a baseline model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task">Task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-loss-optimizer-and-evaluation-metrics">3.1 Setup loss, optimizer and evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-training-loop-and-training-a-model-on-batches-of-data">3.3 Creating a training loop and training a model on batches of data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-predictions-and-get-model-0-results">4. Make predictions and get Model 0 results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one">5. Setup device agnostic-code (for using a GPU if there is one)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1-building-a-better-model-with-non-linearity">6. Model 1: Building a better model with non-linearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">6.1 Setup loss, optimizer and evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#functionizing-training-and-test-loops">6.2 Functionizing training and test loops</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2-building-a-convolutional-neural-network-cnn">7. Model 2: Building a Convolutional Neural Network (CNN)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-model-should-i-use">What model should I use?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stepping-through-nn-conv2d">7.1 Stepping through <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stepping-through-nn-maxpool2d">7.2 Stepping through <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-a-loss-function-and-optimizer-for-model-2">7.3 Setup a loss function and optimizer for <code class="docutils literal notranslate"><span class="pre">model_2</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-model-2-using-our-training-and-test-functions">7.4 Training and testing <code class="docutils literal notranslate"><span class="pre">model_2</span></code> using our training and test functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-model-results-and-training-time">8. Compare model results and training time</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-speed-tradeoff">Performance-speed tradeoff</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-and-evaluate-random-predictions-with-best-model">9. Make and evaluate random predictions with best model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-a-confusion-matrix-for-further-prediction-evaluation">10. Making a confusion matrix for further prediction evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#save-and-load-best-performing-model">11. Save and load best performing model</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Nadezhda Alsahanova
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>